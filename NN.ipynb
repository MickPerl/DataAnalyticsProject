{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: da fare ancora\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from torchinfo import summary\n",
    "from textwrap import dedent\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        df = pd.read_csv(\"script_slurm/df.csv\")\n",
    "        df = self.cleaning(df)\n",
    "\n",
    "        X, y, weights = self.split_XYweights(df)\n",
    "\n",
    "        y = self.discretization(y)\n",
    "\n",
    "        self.num_classes = y.nunique()\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.weights = torch.FloatTensor(weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx], self.weights[idx]\n",
    "\n",
    "    def split_XYweights(self, df):\n",
    "        y = df['rating_mean']\n",
    "        weights = df['ratings_count']\n",
    "        X = df.drop(columns=['ratings_count', 'rating_mean'], axis=1)\n",
    "        return X, y, weights\n",
    "\n",
    "    def cleaning(self, df):\n",
    "        df.dropna(subset = ['rating_mean'], inplace=True)\n",
    "        df_without_tags = df[df.iloc[:, 23:-2].isna().all(axis=1)]\n",
    "        df_without_tags_nor_genres = df_without_tags[df_without_tags['(no genres listed)'] == 1]\n",
    "        rows_to_be_deleted = df.loc[df[\"movieId\"].isin(df_without_tags_nor_genres[\"movieId\"])].index\n",
    "        df.drop(rows_to_be_deleted, axis=0, inplace=True)\n",
    "        df.iloc[:, 23:-2] = df.iloc[:, 23:-2].fillna(0)\n",
    "        df.drop(['(no genres listed)'], inplace=True, axis=1)\n",
    "        df_year_without_na = df.year[-pd.isna(df.year)]\n",
    "        df.year = df.loc[:, 'year'].fillna(np.median(df_year_without_na)).astype('int')\n",
    "        df.drop('movieId', inplace=True, axis=1)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def discretization(self, series):\n",
    "        series = pd.cut(series, bins=5, labels=False)\n",
    "        return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout):\n",
    "        super(Feedforward, self).__init__()\n",
    "    \n",
    "        model = [nn.Linear(input_size, hidden_size), af_first_layer]\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            model.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "            if batch_norm:\n",
    "                model.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            model.append(af_hidden_layers)\n",
    "            \n",
    "            if dropout != 0:\n",
    "                model.append(nn.Dropout(dropout))\n",
    "    \n",
    "\n",
    "        model.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if af_output_layer :\n",
    "            model.append(af_output_layer)\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data_loader, epochs, n_bad_epochs, device):\n",
    "\tmodel.train()\n",
    "\tloss_values = []\n",
    "\tn_bad_epochs = n_bad_epochs\n",
    "\tpatience = 0\n",
    "\tmin_loss = np.Inf\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tlosses_current_batch = []\n",
    "\t\tfor batch_idx, samples in enumerate(data_loader):\n",
    "\t\t\tdata, targets = samples[0].to(device), samples[1].to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# Forward pass\n",
    "\t\t\ty_pred = model(data)\n",
    "\t\t\t# Compute Loss\n",
    "\t\t\tif str(criterion) == \"CrossEntropyLoss()\":\n",
    "\t\t\t\tloss = criterion(y_pred, targets)\n",
    "\t\t\telse:\t# \"KLDivLoss()\"\n",
    "\t\t\t\ttargets = torch.nn.functional.one_hot(targets, num_classes=5).float()\n",
    "\t\t\t\tloss = criterion(y_pred, targets)\n",
    "\n",
    "\t\t\twriter.add_scalar(\"Loss/train\", loss, epoch * len(data_loader) + batch_idx + 1)\n",
    "\t\t\tloss_values.append(loss.item())\n",
    "\t\t\tlosses_current_batch.append(loss.item())\n",
    "\n",
    "\t\t\t# Backward pass\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\tloss_current_batch = np.mean(losses_current_batch)\n",
    "\n",
    "        # If the validation loss is at a minimum\n",
    "\t\tif loss_current_batch < min_loss:\n",
    "\t\t\t# Save the model\n",
    "\t\t\t# torch.save(model)\n",
    "\t\t\tpatience = 0\n",
    "\t\t\tmin_loss = loss_current_batch\n",
    "\t\telse:\n",
    "\t\t\tpatience += 1\n",
    "\n",
    "\t\tprint(f\"Epoch: {epoch}\\t Mean Loss: {loss_current_batch}\\t Current min mean loss: {min_loss}\")\n",
    "\n",
    "\t\tif epoch > 4 and patience > n_bad_epochs:\n",
    "\t\t\tprint(f\"Early stopped at {epoch}-th epoch, since the mean loss over mini-batches didn't decrease during the last {n_bad_epochs} epochs\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn model, loss_values, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "\tmodel.eval()\n",
    "\ty_pred = []\n",
    "\ty_val = []\t\n",
    "\tfor batch_idx, samples in enumerate(data_loader):\n",
    "\t\tdata, targets = samples[0].to(device), samples[1].to(device)\n",
    "\t\ty_pred.append(model(data))\n",
    "\t\ty_val.append(targets)\n",
    "\ty_pred = torch.stack(y_pred).squeeze()\n",
    "\ty_val = torch.stack(y_val).squeeze()\n",
    "\ty_pred = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "\t# classification_report(y_val.cpu(), y_pred.cpu(), zero_division=0)\n",
    "\treport = classification_report(y_val.cpu(), y_pred.cpu(), zero_division=0, output_dict=True)\n",
    "\treturn report\n",
    "\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for batch_idx, samples in enumerate(data_loader):\n",
    "        data, targets = samples[0].to(device), samples[1].to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_test.append(targets)\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "    print(classification_report(y_test.cpu(), y_pred.cpu(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_reproducibility(seed = 42):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\ttorch.use_deterministic_algorithms(True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [1, 3, 5, 7],\n",
    "\t'hidden_size' : [8, 16, 32, 64, 128],\n",
    "\t'batch_size' : [16, 32, 64, 128, 256],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None, nn.LogSoftmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')], \n",
    "\t'dropout' : [0, 0.2, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [0.01, 0.001], \n",
    "\t'optimizer': [\"torch.optim.SGD\", \"torch.optim.Adam\"]\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "dataset = MoviesDataset()\n",
    "train_idx, test_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.y, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=dataset.y[train_idx], random_state=42)\n",
    "\n",
    "# MinMaxScale training, validation and testing set su year e title_length\n",
    "X_train = dataset.X[train_idx]\n",
    "X_val = dataset.X[val_idx]\n",
    "X_test = dataset.X[test_idx]\n",
    "\n",
    "train_year_max = torch.max(X_train[:,1])\n",
    "train_year_min = torch.min(X_train[:,1])\n",
    "dataset.X[train_idx, 1] = (X_train[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[val_idx, 1] = (X_val[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[test_idx, 1] = (X_test[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "\n",
    "train_title_length_max = torch.max(X_train[:,2])\n",
    "train_title_length_min = torch.min(X_train[:,2])\n",
    "dataset.X[train_idx, 2] = (X_train[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[val_idx, 2] = (X_val[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[test_idx, 2] = (X_test[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating samplers to manage unbalancing classes\n",
    "def class_weights(y):\n",
    "    class_count = torch.bincount(y)\n",
    "    class_weighting = 1. / class_count\n",
    "    sample_weights = class_weighting[y]   # sarebbe np.array([weight[t] for t in y_train])\n",
    "    return sample_weights\n",
    "\n",
    "y_train = dataset.y[train_idx]\n",
    "\n",
    "sample_weights = class_weights(y_train)\n",
    "sampler_class_frequency = WeightedRandomSampler(sample_weights, len(train_idx))\n",
    "\n",
    "# MinMaxScaling ratings_count\n",
    "weights_train = dataset.weights[train_idx] \n",
    "weights_val = dataset.weights[val_idx]\n",
    "weights_test = dataset.weights[test_idx] \n",
    "\n",
    "weights_train_max = torch.max(weights_train)\n",
    "weights_train_min = torch.min(weights_train)\n",
    "dataset.weights[train_idx]  = (weights_train - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[val_idx] = (weights_val - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[test_idx] = (weights_test - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "\n",
    "sampler_ratings_count = WeightedRandomSampler(dataset.weights[train_idx], len(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, samples in enumerate(train_loader):\n",
    "#     print(len(np.where(samples[1].numpy() == 0)[0]),\n",
    "#         len(np.where(samples[1].numpy() == 1)[0]),\n",
    "#         len(np.where(samples[1].numpy() == 2)[0]),\n",
    "#         len(np.where(samples[1].numpy() == 3)[0]),\n",
    "#         len(np.where(samples[1].numpy() == 4)[0]), sep = \"\\t\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_cardinality = 0\n",
    "for config_params in itertools.product(*hyperparams.values()):\n",
    "\tfor param in range(len(hyperparams)):\n",
    "\t\tglobals()[names[param]] = config_params[param]\n",
    "\t\n",
    "\tif batch_norm and batch_size < 32 : # non ha significatività statistica\n",
    "\t\tcontinue\n",
    "\n",
    "\tif str(loss_function) == \"CrossEntropyLoss()\" and af_output_layer != None:\n",
    "\t\tcontinue\n",
    "\n",
    "\tif str(loss_function) == \"KLDivLoss()\" and str(af_output_layer) != \"LogSoftmax(dim=1)\":\n",
    "\t\tcontinue\n",
    "\n",
    "\tif dropout == 0.5 and hidden_size < 64 :\n",
    "\t\tcontinue\n",
    "\tif dropout == 0.2 and hidden_size > 32 :\n",
    "\t\tcontinue\n",
    "\t\n",
    "\tconfigs_cardinality += 1\n",
    "\n",
    "count = 0\n",
    "first_configs_set = []\n",
    "second_configs_set = []\n",
    "third_configs_set = []\n",
    "fourth_configs_set = []\n",
    "for config_params in itertools.product(*hyperparams.values()):\n",
    "\tcount += 1\n",
    "\tif count <= (configs_cardinality / 4) :\n",
    "\t\tfirst_configs_set.append(config_params)\n",
    "\telif count > (configs_cardinality / 4) and count <= (configs_cardinality / 2) :\n",
    "\t\tsecond_configs_set.append(config_params)\n",
    "\telif count > (configs_cardinality / 2) and count <= (3 * (configs_cardinality / 4)) :\n",
    "\t\tthird_configs_set.append(config_params)\n",
    "\telif count > (3 * (configs_cardinality / 4)) and count <= configs_cardinality :\n",
    "\t\tfourth_configs_set.append(config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1° training with params:\n",
      "-- num_epochs: 500\n",
      "-- n_bad_epochs: 3\n",
      "-- num_hidden_layers: 1\n",
      "-- hidden_size: 8\n",
      "-- batch_size: 16\n",
      "-- af_first_layer: Tanh()\n",
      "-- af_hidden_layers: LeakyReLU(negative_slope=0.01)\n",
      "-- af_output_layer: None\n",
      "-- loss_function: CrossEntropyLoss()\n",
      "-- dropout: 0\n",
      "-- batch_norm: False\n",
      "-- learning_rate: 0.01\n",
      "-- optimizer: torch.optim.SGD\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "Feedforward                              --                        --                        --\n",
      "├─Sequential: 1-1                        [16, 2240, 1149]          [16, 2240, 5]             --\n",
      "│    └─Linear: 2-1                       [16, 2240, 1149]          [16, 2240, 8]             9,200\n",
      "│    └─Tanh: 2-2                         [16, 2240, 8]             [16, 2240, 8]             --\n",
      "│    └─Linear: 2-3                       [16, 2240, 8]             [16, 2240, 8]             72\n",
      "│    └─LeakyReLU: 2-4                    [16, 2240, 8]             [16, 2240, 8]             --\n",
      "│    └─Linear: 2-5                       [16, 2240, 8]             [16, 2240, 5]             45\n",
      "===================================================================================================================\n",
      "Total params: 9,317\n",
      "Trainable params: 9,317\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.15\n",
      "===================================================================================================================\n",
      "Input size (MB): 164.72\n",
      "Forward/backward pass size (MB): 6.02\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 170.78\n",
      "===================================================================================================================\n",
      "Epoch: 0\t Mean Loss: 1.6110711413834775\t Current min mean loss: 1.6110711413834775\n",
      "Epoch: 1\t Mean Loss: 1.6098809257149695\t Current min mean loss: 1.6098809257149695\n",
      "Epoch: 2\t Mean Loss: 1.609836298440184\t Current min mean loss: 1.609836298440184\n",
      "Epoch: 3\t Mean Loss: 1.6098901187734944\t Current min mean loss: 1.609836298440184\n",
      "Epoch: 4\t Mean Loss: 1.6099185521581343\t Current min mean loss: 1.609836298440184\n",
      "Epoch: 5\t Mean Loss: 1.6098366768764598\t Current min mean loss: 1.609836298440184\n",
      "Epoch: 6\t Mean Loss: 1.6097086883549179\t Current min mean loss: 1.6097086883549179\n",
      "Epoch: 7\t Mean Loss: 1.6097224220101323\t Current min mean loss: 1.6097086883549179\n",
      "Epoch: 8\t Mean Loss: 1.6098635603274618\t Current min mean loss: 1.6097086883549179\n",
      "Epoch: 9\t Mean Loss: 1.6096379545650312\t Current min mean loss: 1.6096379545650312\n",
      "Epoch: 10\t Mean Loss: 1.6097647545593126\t Current min mean loss: 1.6096379545650312\n",
      "Epoch: 11\t Mean Loss: 1.609869056620768\t Current min mean loss: 1.6096379545650312\n",
      "Epoch: 12\t Mean Loss: 1.6097968565034015\t Current min mean loss: 1.6096379545650312\n",
      "Epoch: 13\t Mean Loss: 1.6097471358520643\t Current min mean loss: 1.6096379545650312\n",
      "Early stopped at 13-th epoch, since the mean loss over mini-batches didn't decrease during the last 3 epochs\n",
      "Loss: 1.6118921041488647\n",
      "\n",
      "2° training with params:\n",
      "-- num_epochs: 500\n",
      "-- n_bad_epochs: 3\n",
      "-- num_hidden_layers: 1\n",
      "-- hidden_size: 8\n",
      "-- batch_size: 16\n",
      "-- af_first_layer: Tanh()\n",
      "-- af_hidden_layers: LeakyReLU(negative_slope=0.01)\n",
      "-- af_output_layer: None\n",
      "-- loss_function: CrossEntropyLoss()\n",
      "-- dropout: 0\n",
      "-- batch_norm: False\n",
      "-- learning_rate: 0.01\n",
      "-- optimizer: torch.optim.Adam\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "Feedforward                              --                        --                        --\n",
      "├─Sequential: 1-1                        [16, 2240, 1149]          [16, 2240, 5]             --\n",
      "│    └─Linear: 2-1                       [16, 2240, 1149]          [16, 2240, 8]             9,200\n",
      "│    └─Tanh: 2-2                         [16, 2240, 8]             [16, 2240, 8]             --\n",
      "│    └─Linear: 2-3                       [16, 2240, 8]             [16, 2240, 8]             72\n",
      "│    └─LeakyReLU: 2-4                    [16, 2240, 8]             [16, 2240, 8]             --\n",
      "│    └─Linear: 2-5                       [16, 2240, 8]             [16, 2240, 5]             45\n",
      "===================================================================================================================\n",
      "Total params: 9,317\n",
      "Trainable params: 9,317\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.15\n",
      "===================================================================================================================\n",
      "Input size (MB): 164.72\n",
      "Forward/backward pass size (MB): 6.02\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 170.78\n",
      "===================================================================================================================\n",
      "Epoch: 0\t Mean Loss: 1.6114357098937035\t Current min mean loss: 1.6114357098937035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24026/1104446304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"(model.parameters(), lr=learning_rate)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_bad_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss_values[-1]}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24026/1520934799.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, data_loader, epochs, n_bad_epochs, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mloss_current_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_current_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/mick.perl/DataAnalyticsProject/venv/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/mick.perl/DataAnalyticsProject/venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/mick.perl/DataAnalyticsProject/venv/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/mick.perl/DataAnalyticsProject/venv/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_reproducibility()\n",
    "\n",
    "nr_train = 0\n",
    "*names, = hyperparams\n",
    "columns = [\"nr_train\"] + names + [\"epoch_stopped\", \"loss\", \"accuracy\", \"precision\", \"recall\", \"f1_score\", \"support\"]\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "for config_params in itertools.product(*hyperparams.values()):\n",
    "\tfor param in range(len(hyperparams)):\n",
    "\t\tglobals()[names[param]] = config_params[param]\n",
    "\t\n",
    "\tif batch_norm and batch_size < 32 : # non ha significatività statistica\n",
    "\t\tprint(\"Skipped config with batch_size < 32 and batch norm, since batches aren't statistically significant.\\n\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tif str(loss_function) == \"CrossEntropyLoss()\" and af_output_layer != None:\n",
    "\t\tprint(dedent('''\n",
    "\t\t\tSkipped config with CrossEntropy as loss function and whichever activation function in the output layer,\n",
    "\t\t\tsince CrossEntropy always contains SoftMax as activation function of output layer.\n",
    "\t\t'''))\n",
    "\t\tcontinue\n",
    "\n",
    "\tif str(loss_function) == \"KLDivLoss()\" and str(af_output_layer) != \"LogSoftmax(dim=1)\":\n",
    "\t\tprint(dedent('''\n",
    "\t\t\tSkipped config with Kullback-Leibler divergence as loss function and whichever activation function\n",
    "\t\t\tin the output layer other than SoftMax: since Kullback-Leibler divergence works with probability\n",
    "\t\t\tdistributions, it's suitable the SoftMax as the activation function of the output layer in that it\n",
    "\t\t\treturns a probability distribution over classes for each feature vector in input.\n",
    "\t\t'''))\n",
    "\t\tcontinue\n",
    "\n",
    "\tnr_train += 1\n",
    "\tprint(f\"{nr_train}° training with params:\")\n",
    "\tfor param in range(len(hyperparams)):\n",
    "\t\tprint(f\"-- {names[param]}: {config_params[param]}\")\n",
    "\n",
    "\tname_run = 'Train_' + str(nr_train) + '____' + '__'.join(map(str, config_params))\n",
    "\twriter = SummaryWriter(log_dir=os.path.join('tensorboard_logs', name_run))\n",
    "\t\n",
    "\ttrain_subset = Subset(dataset, train_idx)\n",
    "\tval_subset=Subset(dataset, val_idx)\n",
    "\ttest_subset=Subset(dataset, test_idx)\n",
    "\ttrain_loader=DataLoader(train_subset, batch_size=batch_size, shuffle=False, sampler=sampler_class_frequency, drop_last=True)\n",
    "\tval_loader=DataLoader(val_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\ttest_loader=DataLoader(test_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "\tmodel=Feedforward(dataset.X.shape[1], hidden_size, dataset.num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout)\n",
    "\twriter.add_graph(model, dataset.X)\n",
    "\tmodel.to(device)\n",
    "\tsummary(model, input_size=(batch_size, int(35850/ batch_size),1149), col_names= [\"input_size\",\"output_size\", \"num_params\"], verbose=1)\n",
    "\t# dataset.X[train_idx].shape[1] == 1149, dataset.X[train_idx].shape[0] == 35850\t\t\tprovare verbose = 2 per weight e bias\n",
    "\t# evaluate_model(model, val_loader, device)\n",
    "\n",
    "\tloss_func = loss_function \n",
    "\n",
    "\toptim = eval(optimizer + \"(model.parameters(), lr=learning_rate)\")\n",
    "\tmodel, loss_values, epoch_stopped = train_model(model, loss_func, optim, train_loader, num_epochs, n_bad_epochs, device)\n",
    "\tprint(f\"Loss: {loss_values[-1]}\", end=\"\\n\\n\")\n",
    "\twriter.flush()\n",
    "\twriter.close()\n",
    "\n",
    "\treport = evaluate_model(model, val_loader, device)\n",
    "\tindex_classes = len(report) - 3\n",
    "\tf1_score = [report[str(i)]['f1-score'] for i in range(index_classes)]\n",
    "\tprecision = [report[str(i)]['precision'] for i in range(index_classes)]\n",
    "\trecall = [report[str(i)]['recall'] for i in range(index_classes)]\n",
    "\tsupport = [report[str(i)]['support'] for i in range(index_classes)]\n",
    "\taccuracy = report['accuracy']\n",
    "\trow_values= [nr_train] + list(map(str, config_params)) + [epoch_stopped, loss_values[-1], accuracy, precision, recall, f1_score, support]\n",
    "\tresults=results.append(pd.Series(row_values, index=columns), ignore_index=True)\n",
    "\t# plt.plot(loss_values)\n",
    "\t# plt.title(\"Number of epochs: {}\".format(num_epochs))\n",
    "\t# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nr_train</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>af_first_layer</th>\n",
       "      <th>af_hidden_layers</th>\n",
       "      <th>af_output_layer</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>epoch_stopped</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nr_train, num_epochs, num_hidden_layers, hidden_size, batch_size, af_first_layer, af_hidden_layers, af_output_layer, loss_function, dropout, batch_norm, learning_rate, optimizer, epoch_stopped, loss, accuracy, precision, recall, f1_score, support]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"tensorboard_logs/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, model):\n",
    "    row = torch.Tensor([row])\n",
    "    yhat = model(row)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
