{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from torchinfo import summary\n",
    "from textwrap import dedent\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to exploit the parallel computing offered by CUDA on the GPU, if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function assures the reproducibility of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed = 42):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\ttorch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By instatiating this class, we load the dataset in output by the *Data Manipulation* section of the notebook, we clean it with the same operations done in the *Data Cleaning section*, we split it into `X`, `y` and `ratings_count` as `weights` and, eventually, we discretize the continuous label into 5 discrete classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviesDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\ttry: \n",
    "\t\t\tdf = pd.read_csv(\"datasets/df.csv\")\n",
    "\t\t\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint(f\"Download in progress of df.csv\")\n",
    "\t\t\tfile, _ = urlretrieve(url = \"http://github.com/MickPerl/DataAnalyticsProject/releases/download/datasets/df.csv\", filename=\"datasets/df.csv\")\n",
    "\t\t\tdf = pd.read_csv(file)\n",
    "\n",
    "\t\tdf = pd.read_csv(\"datasets/df.csv\")\n",
    "\t\tdf = self.cleaning(df)\n",
    "\n",
    "\t\tX, y, weights = self.split_XYweights(df)\n",
    "\n",
    "\t\ty = self.discretization(y)\n",
    "\n",
    "\t\tself.num_classes = y.nunique()\n",
    "\t\tself.X = torch.FloatTensor(X.values)\n",
    "\t\tself.y = torch.LongTensor(y)\n",
    "\t\tself.weights = torch.FloatTensor(weights)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.X.shape[0]\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.X[idx, :], self.y[idx], self.weights[idx]\n",
    "\n",
    "\tdef split_XYweights(self, df):\n",
    "\t\ty = df['rating_mean']\n",
    "\t\tweights = df['ratings_count']\n",
    "\t\tX = df.drop(columns=['ratings_count', 'rating_mean'], axis=1)\n",
    "\t\treturn X, y, weights\n",
    "\n",
    "\tdef cleaning(self, df):\n",
    "\t\tdf.dropna(subset = ['rating_mean'], inplace=True)\n",
    "\t\tdf_without_tags = df[df.iloc[:, 23:-2].isna().all(axis=1)]\n",
    "\t\tdf_without_tags_nor_genres = df_without_tags[df_without_tags['(no genres listed)'] == 1]\n",
    "\t\trows_to_be_deleted = df.loc[df[\"movieId\"].isin(df_without_tags_nor_genres[\"movieId\"])].index\n",
    "\t\tdf.drop(rows_to_be_deleted, axis=0, inplace=True)\n",
    "\t\tdf.iloc[:, 23:-2] = df.iloc[:, 23:-2].fillna(0)\n",
    "\t\tdf.drop(['(no genres listed)'], inplace=True, axis=1)\n",
    "\t\tdf_year_without_na = df.year[-pd.isna(df.year)]\n",
    "\t\tdf.year = df.loc[:, 'year'].fillna(np.median(df_year_without_na)).astype('int')\n",
    "\t\tdf.drop('movieId', inplace=True, axis=1)\n",
    "\t\tdf.drop_duplicates(inplace=True)\n",
    "\t\treturn df\n",
    "\n",
    "\tdef discretization(self, series):\n",
    "\t\treturn pd.cut(series, bins=5, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By instiantiating this class, we build the network architecture.\\\n",
    "The architecture is highly parametrized: in particular, some of the parameters that it is possible to specify are the activation functions of the first layer, that of the hidden layers and that of the output layer as well as the number of hidden layers, the probability of dropout and batch normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout, batch_norm):\n",
    "        super(Feedforward, self).__init__()\n",
    "    \n",
    "        model = [nn.Linear(input_size, hidden_size), af_first_layer]\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            model.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "            if batch_norm:\n",
    "                model.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            model.append(af_hidden_layers)\n",
    "            \n",
    "            if dropout != 0:\n",
    "                model.append(nn.Dropout(dropout))\n",
    "    \n",
    "\n",
    "        model.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if af_output_layer :\n",
    "            model.append(af_output_layer)\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement by hand the **early stopping** mechanism; in detail, we trigger it after the fifth epoch and we set to 3 the number of consecutive epochs we tolerate an increase of the loss (`n_bad_epochs`): every time the loss decreases with respect to the last min value, the counter of bad epochs is reset.\n",
    "\n",
    "We log on **TensorBoard** some values such as the loss and the accuracy every batch, the loss and the accuracy every epoch as well as the weights and the bias every batch.\n",
    "\n",
    "Furthermore, we check for the **vanishing and exploding gradient phenomenon**; even thought the architecture is well designed, there could be some batch containing bad examples which cause a na or inf gradient: ideally, these samples should be removed, but we solely skip them and continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def train_model(model, criterion, optimizer, data_loader, epochs, n_bad_epochs, device, tb, cardinality_training_set):\n",
    "\tmodel.train()\n",
    "\n",
    "\tloss_values = []\t# to store loss values over all batches regardless distinct epochs: it's the list we return after training\n",
    "\n",
    "\tn_bad_epochs = n_bad_epochs\n",
    "\tpatience = 0\n",
    "\tmin_loss = np.Inf\n",
    "\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tlosses_batches_current_epoch = []\t# to store loss values over all batches with regard to a single epoch to checking condition about early stopping\n",
    "\t\tcorrect_batches_current_epoch = []\n",
    "\t\t\n",
    "\t\tfor batch_idx, samples in enumerate(data_loader):\n",
    "\t\t\tdata, targets = samples[0].to(device), samples[1].to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\ty_pred = model(data)\n",
    "\t\t\t\n",
    "\t\t\tif str(criterion) == \"CrossEntropyLoss()\":\n",
    "\t\t\t\tloss = criterion(y_pred, targets)\n",
    "\t\t\telse:\t# \"KLDivLoss()\"\n",
    "\t\t\t\ttargets_one_hot_encoded = torch.nn.functional.one_hot(targets, num_classes=5).float()\n",
    "\t\t\t\tloss = criterion(y_pred, targets_one_hot_encoded)\n",
    "\n",
    "\t\t\tcorrect = get_num_correct(y_pred, targets)\n",
    "\t\t\t\n",
    "\t\t\ttb.add_scalar(\"Loss every batch\", loss, epoch * len(data_loader) + batch_idx + 1)\n",
    "\t\t\ttb.add_scalar(\"Correct every batch\", correct, epoch * len(data_loader) + batch_idx + 1)\n",
    "\t\t\ttb.add_scalar(\"Accuracy every batch\", correct / len(data), epoch * len(data_loader) + batch_idx + 1)\n",
    "\n",
    "\t\t\tloss_values.append(loss.item())\n",
    "\t\t\tlosses_batches_current_epoch.append(loss.item())\n",
    "\t\t\tcorrect_batches_current_epoch.append(correct)\n",
    "\n",
    "\t\t\t# Backward pass\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tvalid_gradients = True\n",
    "\t\t\tfor name, param in model.named_parameters():\n",
    "\t\t\t\tif param.grad is not None:\n",
    "\t\t\t\t\tif torch.isnan(param.grad).any():\n",
    "\t\t\t\t\t\tprint(f\"{name} is nan, so model parameters are not going to be updated: this batch is skipped and the gradient is reset.\")\n",
    "\t\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\t\tvalid_gradients = False\n",
    "\t\t\t\t\tif torch.isinf(param.grad).any():\n",
    "\t\t\t\t\t\tprint(f\"{name} is inf, so model parameters are not going to be updated: this batch is skipped and the gradient is reset.\")\n",
    "\t\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\t\tvalid_gradients = False\n",
    "\t\t\tif not valid_gradients :\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tfor name, value in model.named_parameters():\n",
    "\t\t\t\tname = name.replace('.', '/')\n",
    "\t\t\t\ttb.add_histogram('every batch_' + name, param.data.cpu().detach().numpy(), batch_idx + 1)\n",
    "\t\t\t\ttb.add_histogram('every batch_' + name + '/grad', param.grad.data.cpu().numpy(), batch_idx + 1)\n",
    "\t\t\n",
    "\t\ttotal_correct_current_epoch = np.sum(correct_batches_current_epoch)\n",
    "\t\ttb.add_scalar(\"Correct every epoch\", total_correct_current_epoch, epoch)\n",
    "\n",
    "\t\taccuracy_current_epoch = total_correct_current_epoch / cardinality_training_set\n",
    "\t\ttb.add_scalar(\"Accuracy every epoch\", accuracy_current_epoch, epoch)\n",
    "\t\t\n",
    "\t\tfor name, param in model.named_parameters():\n",
    "\t\t\tname = name.replace('.', '/')\n",
    "\t\t\ttb.add_histogram('every epoch_' + name, param.data.cpu().detach().numpy(), epoch)\n",
    "\t\t\ttb.add_histogram('every epoch_' + name + '/grad', param.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "\t\tmean_loss_current_epoch = np.mean(losses_batches_current_epoch)\n",
    "\t\ttb.add_scalar(\"Loss every epoch\", mean_loss_current_epoch, epoch)\n",
    "\n",
    "\t\tif epoch < 5 :\n",
    "\t\t\tprint(f\"Epoch: {epoch}\\t Mean Loss: {mean_loss_current_epoch}\")\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif epoch == 5 :\n",
    "\t\t\tprint(\"Waiting for three consecutive epochs during which the mean loss over batches does not decrease...\")\n",
    "        \n",
    "\t\tif mean_loss_current_epoch < min_loss:\n",
    "\t\t\t# Save the model\n",
    "\t\t\t# torch.save(model)\n",
    "\t\t\tpatience = 0\n",
    "\t\t\tmin_loss = mean_loss_current_epoch\n",
    "\t\telse:\n",
    "\t\t\tpatience += 1\n",
    "\n",
    "\t\tprint(f\"Epoch: {epoch}\\t Mean Loss: {mean_loss_current_epoch}\\t Current min mean loss: {min_loss}\")\n",
    "\n",
    "\t\tif patience == n_bad_epochs:\n",
    "\t\t\tprint(f\"Early stopped at {epoch}-th epoch, since the mean loss over batches didn't decrease during the last {n_bad_epochs} epochs\")\n",
    "\t\t\treturn model, loss_values, epoch, mean_loss_current_epoch, accuracy_current_epoch\n",
    "\n",
    "\n",
    "\treturn model, loss_values, epoch, mean_loss_current_epoch, accuracy_current_epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, device, output_dict = False):\n",
    "\tmodel.eval()\n",
    "\ty_pred = []\n",
    "\ty_test = []\n",
    "\t\n",
    "\tfor batch_idx, samples in enumerate(data_loader):\n",
    "\t    data, targets = samples[0].to(device), samples[1].to(device)\n",
    "\t    y_pred.append(model(data))\n",
    "\t    y_test.append(targets)\n",
    "\ty_pred = torch.stack(y_pred).squeeze()\n",
    "\ty_test = torch.stack(y_test).squeeze()\n",
    "\ty_pred = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "\treturn classification_report(y_test.cpu(), y_pred.cpu(), zero_division=0, output_dict=output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following utility function lets us to obtain the samples' weights from the classes' weights: this output are going to be used in the sampler of the `DataLoader` object in order to manage the data imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(y):\n",
    "    class_count = torch.bincount(y)\n",
    "    class_weighting = 1. / class_count\n",
    "    sample_weights = class_weighting[y]   # np.array([weighting[t] for t in y_train])\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a bug in the TensorBoard porting to PyTorch, we inherit the `SummaryWriter` class and overwrite the `add_hparams` function with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryWriter(SummaryWriter):\n",
    "\n",
    "\tdef add_hparams(self, hparam_dict, metric_dict):\n",
    "\t\ttorch._C._log_api_usage_once(\"tensorboard.logging.add_hparams\")\n",
    "\t\tif type(hparam_dict) is not dict or type(metric_dict) is not dict:\n",
    "\t\t\traise TypeError('hparam_dict and metric_dict should be dictionary.')\n",
    "\t\texp, ssi, sei = hparams(hparam_dict, metric_dict)\n",
    "\n",
    "\t\tself.file_writer.add_summary(exp)\n",
    "\t\tself.file_writer.add_summary(ssi)\n",
    "\t\tself.file_writer.add_summary(sei)\n",
    "\t\tfor k, v in metric_dict.items():\n",
    "\t\t\tif v is not None:\n",
    "\t\t\t\tself.add_scalar(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to extract dictionaries containing a hyperparameters' configuration from the cartesian product of values of the hyperparameters; in detail, before creating a dictionary we check some condition in order to skip pointless or incorrect configurations.\\\n",
    "Examples of skipped configurations are those with:\n",
    "- batch_size < 32 and batch norm, since batches aren't statistically significant;\n",
    "- CrossEntropy as loss function and whichever activation function in the output layer, since CrossEntropy always contains SoftMax as activation function of output layer;\n",
    "- Kullback-Leibler divergence as loss function and whichever activation function in the output layer other than SoftMax: since Kullback-Leibler divergence works with probability distributions, the SoftMax as the activation function of the output layer is a suitable choice in that it returns a probability distribution over classes for each feature vector in input.\n",
    "- high probability of dropout (0.5) and a hidden layer sizes less than 64;\n",
    "- low probability of dropout (0.2) and hidden layer size greater than 32;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_configs_from_params_cartesian_product(hyperparams) :\n",
    "\tname_params = list(hyperparams.keys())\n",
    "\tcartesian_product_filtered = []\n",
    "\tcartesian_product_config_params = itertools.product(*hyperparams.values())\n",
    "\n",
    "\tfor conf_params in cartesian_product_config_params:\n",
    "\t\tconf_params_dict = {name_params[i]: conf_params[i] for i in range(len(hyperparams))}\n",
    "\t\t\n",
    "\t\tif conf_params_dict['batch_norm'] and conf_params_dict['batch_size'] < 32 :\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif str(conf_params_dict['loss_function']) == \"CrossEntropyLoss()\" and conf_params_dict['af_output_layer'] != None:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif str(conf_params_dict['loss_function']) == \"KLDivLoss()\" and str(conf_params_dict['af_output_layer']) != \"LogSoftmax(dim=1)\":\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif conf_params_dict['dropout'] == 0.5 and conf_params_dict['hidden_size'] < 64 :\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif conf_params_dict['dropout'] == 0.2 and conf_params_dict['hidden_size'] > 32 :\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tcartesian_product_filtered.append(conf_params_dict)\n",
    "\t\n",
    "\treturn cartesian_product_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of parameters' configurations are really high (~ 6000), we implement a function to split them into `nr_sets` subsets: so that, we are able to execute the hyperparameters optimization in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_configs_params(dict_configs, nr_sets = 4):\n",
    "\tassert len(dict_configs) % nr_sets == 0,  \"The number of configs params sets have to be a dividend of the cardinality of all configs.\"\n",
    "\tprint(f\"Newly created sets (ratio {nr_sets}:1 to all {len(dict_configs)} configs):\")\n",
    "\n",
    "\tfor i in range(nr_sets):\n",
    "\t\tglobals()[f\"configs_set{i}\"] = np.array_split(dict_configs, nr_sets)[i]\n",
    "\t\tprint(f\"configs_set{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoviesDataset()\n",
    "train_idx, test_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.y, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=dataset.y[train_idx], random_state=42)\n",
    "\n",
    "X_train = dataset.X[train_idx]\n",
    "X_val = dataset.X[val_idx]\n",
    "X_test = dataset.X[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We min-max scale `year` e `title_length` on training, validation and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_year_max = torch.max(X_train[:,0])\n",
    "train_year_min = torch.min(X_train[:,0])\n",
    "dataset.X[train_idx, 0] = (X_train[:,0] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[val_idx, 0] = (X_val[:,0] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[test_idx, 0] = (X_test[:,0] - train_year_min)/(train_year_max - train_year_min)\n",
    "\n",
    "train_title_length_max = torch.max(X_train[:,1])\n",
    "train_title_length_min = torch.min(X_train[:,1])\n",
    "dataset.X[train_idx, 1] = (X_train[:,1] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[val_idx, 1] = (X_val[:,1] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[test_idx, 1] = (X_test[:,1] - train_title_length_min)/(train_title_length_max - train_title_length_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two samplers which we are going to pass to the `DataLoader` object in order to manage the data imbalance: \n",
    "- `sampler_class_frequency` which, as its name reveals, weights each sample depending on the frequency of the class it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dataset.y[train_idx]\n",
    "\n",
    "sample_weights = class_weights(y_train)\n",
    "sampler_class_frequency = WeightedRandomSampler(sample_weights, len(train_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the classes' distribution over a subsets of batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "26\t22\t25\t28\t27\n",
      "35\t18\t28\t26\t21\n",
      "29\t24\t27\t27\t21\n",
      "23\t28\t28\t27\t22\n",
      "28\t22\t23\t23\t32\n",
      "28\t30\t18\t31\t21\n",
      "31\t21\t18\t29\t29\n",
      "20\t28\t36\t13\t31\n",
      "22\t27\t29\t26\t24\n",
      "17\t23\t29\t30\t29\n",
      "31\t25\t22\t28\t22\n",
      "39\t19\t19\t24\t27\n",
      "30\t27\t25\t28\t18\n",
      "31\t17\t28\t20\t32\n",
      "27\t22\t29\t29\t21\n",
      "23\t29\t35\t22\t19\n",
      "28\t30\t23\t24\t23\n",
      "22\t29\t30\t23\t24\n",
      "21\t28\t25\t32\t22\n",
      "31\t23\t25\t22\t27\n",
      "22\t35\t23\t18\t30\n",
      "24\t28\t16\t31\t29\n",
      "25\t30\t24\t28\t21\n"
     ]
    }
   ],
   "source": [
    "train_subset = Subset(dataset, train_idx)\n",
    "print(type(train_subset))\n",
    "train_loader=DataLoader(train_subset, batch_size=128, shuffle=False, sampler=sampler_class_frequency, drop_last=True)\n",
    "\n",
    "for i, samples in enumerate(train_loader):\n",
    "\tif not i%10:\n",
    "\t\tprint(len(np.where(samples[1].numpy() == 0)[0]),\n",
    "        len(np.where(samples[1].numpy() == 1)[0]),\n",
    "        len(np.where(samples[1].numpy() == 2)[0]),\n",
    "        len(np.where(samples[1].numpy() == 3)[0]),\n",
    "        len(np.where(samples[1].numpy() == 4)[0]), sep = \"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sampler_ratings_count` which weights each sample depending on the `ratings_count` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaling ratings_count\n",
    "weights_train = dataset.weights[train_idx] \n",
    "weights_val = dataset.weights[val_idx]\n",
    "weights_test = dataset.weights[test_idx] \n",
    "\n",
    "weights_train_max = torch.max(weights_train)\n",
    "weights_train_min = torch.min(weights_train)\n",
    "dataset.weights[train_idx]  = (weights_train - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[val_idx] = (weights_val - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[test_idx] = (weights_test - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "\n",
    "sampler_ratings_count = WeightedRandomSampler(dataset.weights[train_idx], len(train_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have conducted some experiments with both and the performance have consistently been better with `sampler_class_frequency`, therefore we have ever adopted it during the following fine tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining first hyperparameters space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the first hyperparameters optimization, we set the number of epochs to a very high value (500) as the early stopping assures that the training continues as long as the loss decreases and no further (in detail, the patience is set to 3). For remaining hyperparameters we define a wide space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [1, 3, 5, 7],\n",
    "\t'hidden_size' : [8, 16, 32, 64, 128],\n",
    "\t'batch_size' : [16, 32, 64, 128, 256],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None, nn.LogSoftmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')], \n",
    "\t'dropout' : [0, 0.2, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [0.01, 0.001], \n",
    "\t'optimizer': [\"torch.optim.SGD\", \"torch.optim.Adam\"],\n",
    "\t'weight_decay': [1e-4]\t\t\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the parameters' configurations into 6 sets and then we execute scripts specifying the index of the sets we want to consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_configs = dict_configs_from_params_cartesian_product(first_hyperparams)\n",
    "nr_sets = 6\n",
    "split_configs_params(first_configs, nr_sets)\n",
    "\n",
    "idx_set = 1\n",
    "assert idx_set < nr_sets, f\"You can specify a set with an index until {nr_sets-1}\"\n",
    "config_set = eval(f\"configs_set{idx_set}\") \n",
    "\n",
    "if config_set == first_configs:\n",
    "\tnr_train = 0\n",
    "else :\n",
    "\tnr_train = len(configs_set0) * idx_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log to TensorBoard the architecture of the network and the various hyperparameters' configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_reproducibility()\t\n",
    "\t\n",
    "columns = [\"nr_train\"] + list(first_configs[0].keys()) + [\"epoch_stopped\", \"loss\", \"accuracy\", \"precision\", \"precision_total\", \"recall\", \"recall_total\", \"f1_score\", \"f1_score_total\", \"support\"]\n",
    "results_first_ft = pd.DataFrame(columns=columns)\n",
    "\n",
    "for config_params in config_set:\n",
    "\tnr_train += 1\n",
    "\tprint(f\"{nr_train}° training with params:\")\n",
    "\tpprint(config_params)\n",
    "\n",
    "\tlist_params_config = list(map(str, list(config_params.values())))\n",
    "\tname_run = '__'.join(list_params_config)\n",
    "\twith SummaryWriter(log_dir=os.path.join('tensorboard_logs', f\"{idx_set}_out_of_{nr_sets - 1}\", 'Train_' + str(nr_train), name_run)) as tb:\n",
    "\t# tb = SummaryWriter(log_dir=os.path.join('tensorboard_logs', f\"{idx_set}_out_of_{nr_sets - 1}\", 'Train_' + str(nr_train), name_run))\n",
    "\n",
    "\t\ttrain_subset = Subset(dataset, train_idx)\n",
    "\t\tval_subset=Subset(dataset, val_idx)\n",
    "\t\ttest_subset=Subset(dataset, test_idx)\n",
    "\t\ttrain_loader=DataLoader(train_subset, batch_size=config_params['batch_size'], shuffle=False, sampler=sampler_class_frequency, drop_last=True)\n",
    "\t\tval_loader=DataLoader(val_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\t\ttest_loader=DataLoader(test_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "\t\tmodel = Feedforward(\n",
    "\t\t\tdataset.X.shape[1],\n",
    "\t\t\tconfig_params['hidden_size'],\n",
    "\t\t\tdataset.num_classes,\n",
    "\t\t\tconfig_params['af_first_layer'],\n",
    "\t\t\tconfig_params['af_hidden_layers'],\n",
    "\t\t\tconfig_params['af_output_layer'],\n",
    "\t\t\tconfig_params['num_hidden_layers'],\n",
    "\t\t\tconfig_params['dropout'], \n",
    "\t\t\tconfig_params['batch_norm'])\n",
    "\n",
    "\t\tmodel.to(device)\n",
    "\t\tinput_model = dataset.X[train_idx][:config_params['batch_size']].to(device)\n",
    "\t\ttb.add_graph(model, input_model)\n",
    "\n",
    "\t\tsummary(model, input_size=(config_params['batch_size'], int(35850 // config_params['batch_size']), 1149), col_names= [\"input_size\",\"output_size\", \"num_params\"], verbose=1)\n",
    "\n",
    "\t\tloss_func = config_params['loss_function'] \n",
    "\n",
    "\t\toptim = eval(config_params['optimizer'] + \"(model.parameters(), lr=config_params['learning_rate'])\")\n",
    "\n",
    "\t\tcardinality_training_set = len(X_train)\n",
    "\t\tmodel, loss_values, epoch_stopped, loss_value_last_epoch, accuracy_last_epoch = train_model(model, loss_func, optim, train_loader, config_params['num_epochs'], config_params['n_bad_epochs'], device, tb, cardinality_training_set)\n",
    "\t\t\n",
    "\t\tprint(f\"Loss: {loss_value_last_epoch}\", end=\"\\n\\n\")\n",
    "\n",
    "\t\treport = test_model(model, val_loader, device, True)\n",
    "\t\tindex_classes = len(report) - 3\n",
    "\t\tf1_score = [float(report[str(i)]['f1-score']) for i in range(index_classes)]\n",
    "\t\tf1_score_total = np.sum(f1_score)\n",
    "\t\tprecision = [float(report[str(i)]['precision']) for i in range(index_classes)]\n",
    "\t\tprecision_total = np.sum(precision)\n",
    "\t\trecall = [float(report[str(i)]['recall']) for i in range(index_classes)]\n",
    "\t\trecall_total = np.sum(recall)\n",
    "\t\tsupport = [int(report[str(i)]['support']) for i in range(index_classes)]\n",
    "\t\taccuracy = report['accuracy']\n",
    "\n",
    "\n",
    "\t\trow_values= [nr_train] + list_params_config + [epoch_stopped, loss_value_last_epoch, accuracy, precision, precision_total, recall, recall_total, f1_score, f1_score_total, support]\n",
    "\t\tresults_first_ft=results_first_ft.append(pd.Series(row_values, index=columns), ignore_index=True)\n",
    "\n",
    "\t\tdict_params_config = {list(config_params.keys())[z]: list_params_config[z] for z in range(len(config_params))}\n",
    "\t\ttb.add_hparams(hparam_dict = dict_params_config, metric_dict = {\"Accuracy every epoch\": None, \"Loss every epoch\": None})\n",
    "\t\ttb.flush()\n",
    "\t\ttb.close()\n",
    "\tdel model, optim, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_set == first_configs:\n",
    "\tresults_first_ft.to_csv(\"tuning_hyperparams/results_first_ft.csv\", index=False)\n",
    "else :\n",
    "\tresults_first_ft.to_csv(f\"tuning_hyperparams/results_nrSets{nr_sets}_idxSet{idx_set}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first_ft = pd.concat([pd.read_csv(f\"results_hyperparams_optimization/NN/results_nrSets6_idxSet{i}.csv\") for i in range(6)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first_ft.to_csv(\"tuning_hyperparams/results_first_ft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first_ft = pd.read_csv(\"tuning_hyperparams/results_first_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in descending order by accuracy: we note null precisions and recalls regarding class with lower frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>1.715587</td>\n",
       "      <td>0.523795</td>\n",
       "      <td>[0.0, 0.0, 0.74194, 0.49482, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.24147, 0.97881, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.36436, 0.65734, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>1.609664</td>\n",
       "      <td>0.475260</td>\n",
       "      <td>[0.0, 0.0, 0.39234, 0.68404, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.77953, 0.43573, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.52197, 0.53236, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>32.183064</td>\n",
       "      <td>0.453829</td>\n",
       "      <td>[0.07336, 0.0, 0.41394, 0.72903, 0.07955]</td>\n",
       "      <td>[0.15574, 0.0, 0.74278, 0.39901, 0.04459]</td>\n",
       "      <td>[0.09974, 0.0, 0.53162, 0.51575, 0.05714]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>1.609313</td>\n",
       "      <td>0.452884</td>\n",
       "      <td>[0.0, 0.30435, 0.0, 0.45847, 0.0]</td>\n",
       "      <td>[0.0, 0.10448, 0.0, 0.99011, 0.0]</td>\n",
       "      <td>[0.0, 0.15556, 0.0, 0.62673, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>1.609575</td>\n",
       "      <td>0.446896</td>\n",
       "      <td>[0.0, 0.0, 0.37011, 0.60171, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.68679, 0.44703, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.481, 0.51297, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>1.609671</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>1.609638</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>1.612220</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>1.609482</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>1.609466</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss  accuracy                                  precision  \\\n",
       "4041   1.715587  0.523795          [0.0, 0.0, 0.74194, 0.49482, 0.0]   \n",
       "4248   1.609664  0.475260          [0.0, 0.0, 0.39234, 0.68404, 0.0]   \n",
       "5489  32.183064  0.453829  [0.07336, 0.0, 0.41394, 0.72903, 0.07955]   \n",
       "3000   1.609313  0.452884          [0.0, 0.30435, 0.0, 0.45847, 0.0]   \n",
       "2442   1.609575  0.446896          [0.0, 0.0, 0.37011, 0.60171, 0.0]   \n",
       "3016   1.609671  0.446265              [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "3498   1.609638  0.446265              [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "3034   1.612220  0.446265              [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "5282   1.609482  0.446265              [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "4626   1.609466  0.446265              [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "\n",
       "                                         recall  \\\n",
       "4041          [0.0, 0.0, 0.24147, 0.97881, 0.0]   \n",
       "4248          [0.0, 0.0, 0.77953, 0.43573, 0.0]   \n",
       "5489  [0.15574, 0.0, 0.74278, 0.39901, 0.04459]   \n",
       "3000          [0.0, 0.10448, 0.0, 0.99011, 0.0]   \n",
       "2442          [0.0, 0.0, 0.68679, 0.44703, 0.0]   \n",
       "3016                  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "3498                  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "3034                  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "5282                  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "4626                  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "\n",
       "                                       f1_score                      support  \n",
       "4041          [0.0, 0.0, 0.36436, 0.65734, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "4248          [0.0, 0.0, 0.52197, 0.53236, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "5489  [0.09974, 0.0, 0.53162, 0.51575, 0.05714]  [122, 335, 1143, 1416, 157]  \n",
       "3000          [0.0, 0.15556, 0.0, 0.62673, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "2442            [0.0, 0.0, 0.481, 0.51297, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "3016              [0.0, 0.0, 0.0, 0.61713, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "3498              [0.0, 0.0, 0.0, 0.61713, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "3034              [0.0, 0.0, 0.0, 0.61713, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "5282              [0.0, 0.0, 0.0, 0.61713, 0.0]  [122, 335, 1143, 1416, 157]  \n",
       "4626              [0.0, 0.0, 0.0, 0.61713, 0.0]  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_first_ft.sort_values(by=['accuracy'], ascending=False).iloc[:10, -6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in ascending order by loss: the precision and recall regarding class with lower frequency are still quite imbalanced with respect to class with higher frequency but to a lesser extent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>0.859668</td>\n",
       "      <td>0.410652</td>\n",
       "      <td>[0.06918, 0.22021, 0.6506, 0.80118, 0.09045]</td>\n",
       "      <td>[0.27049, 0.25373, 0.37795, 0.48093, 0.4586]</td>\n",
       "      <td>[0.11018, 0.23578, 0.47814, 0.60106, 0.1511]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>0.889824</td>\n",
       "      <td>0.414119</td>\n",
       "      <td>[0.07783, 0.21042, 0.61134, 0.80196, 0.11001]</td>\n",
       "      <td>[0.27049, 0.31343, 0.3867, 0.46328, 0.49682]</td>\n",
       "      <td>[0.12088, 0.2518, 0.47374, 0.58729, 0.18014]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5751</th>\n",
       "      <td>0.899446</td>\n",
       "      <td>0.414434</td>\n",
       "      <td>[0.07598, 0.2367, 0.61605, 0.78498, 0.09413]</td>\n",
       "      <td>[0.30328, 0.26567, 0.3762, 0.48729, 0.43949]</td>\n",
       "      <td>[0.12151, 0.25035, 0.46714, 0.60131, 0.15506]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>0.901097</td>\n",
       "      <td>0.414434</td>\n",
       "      <td>[0.07246, 0.22685, 0.64984, 0.81316, 0.09857]</td>\n",
       "      <td>[0.2459, 0.29254, 0.36045, 0.4887, 0.52866]</td>\n",
       "      <td>[0.11194, 0.25554, 0.4637, 0.6105, 0.16617]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>0.901097</td>\n",
       "      <td>0.409392</td>\n",
       "      <td>[0.0751, 0.24148, 0.65263, 0.82911, 0.10116]</td>\n",
       "      <td>[0.31148, 0.25373, 0.3797, 0.46257, 0.55414]</td>\n",
       "      <td>[0.12102, 0.24745, 0.48009, 0.59383, 0.17109]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>0.907835</td>\n",
       "      <td>0.424519</td>\n",
       "      <td>[0.08578, 0.21218, 0.6767, 0.80765, 0.11053]</td>\n",
       "      <td>[0.31148, 0.30149, 0.37358, 0.49223, 0.53503]</td>\n",
       "      <td>[0.13451, 0.24908, 0.4814, 0.61167, 0.18321]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>0.912362</td>\n",
       "      <td>0.414434</td>\n",
       "      <td>[0.08416, 0.23211, 0.63663, 0.83014, 0.10101]</td>\n",
       "      <td>[0.27869, 0.35821, 0.37708, 0.45904, 0.50955]</td>\n",
       "      <td>[0.12928, 0.28169, 0.47363, 0.59118, 0.1686]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>0.912675</td>\n",
       "      <td>0.417271</td>\n",
       "      <td>[0.07919, 0.22922, 0.63526, 0.80392, 0.1026]</td>\n",
       "      <td>[0.28689, 0.27164, 0.3657, 0.49223, 0.52866]</td>\n",
       "      <td>[0.12411, 0.24863, 0.46419, 0.6106, 0.17184]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>0.918037</td>\n",
       "      <td>0.402143</td>\n",
       "      <td>[0.07795, 0.20388, 0.69039, 0.79138, 0.10307]</td>\n",
       "      <td>[0.33607, 0.25075, 0.33946, 0.47952, 0.53503]</td>\n",
       "      <td>[0.12654, 0.2249, 0.45513, 0.59719, 0.17284]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>0.920030</td>\n",
       "      <td>0.415380</td>\n",
       "      <td>[0.07743, 0.22654, 0.67213, 0.79472, 0.101]</td>\n",
       "      <td>[0.28689, 0.29552, 0.35871, 0.48941, 0.51592]</td>\n",
       "      <td>[0.12195, 0.25648, 0.46777, 0.60577, 0.16893]</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy                                      precision  \\\n",
       "5587  0.859668  0.410652   [0.06918, 0.22021, 0.6506, 0.80118, 0.09045]   \n",
       "2627  0.889824  0.414119  [0.07783, 0.21042, 0.61134, 0.80196, 0.11001]   \n",
       "5751  0.899446  0.414434   [0.07598, 0.2367, 0.61605, 0.78498, 0.09413]   \n",
       "5735  0.901097  0.414434  [0.07246, 0.22685, 0.64984, 0.81316, 0.09857]   \n",
       "4115  0.901097  0.409392   [0.0751, 0.24148, 0.65263, 0.82911, 0.10116]   \n",
       "2691  0.907835  0.424519   [0.08578, 0.21218, 0.6767, 0.80765, 0.11053]   \n",
       "4067  0.912362  0.414434  [0.08416, 0.23211, 0.63663, 0.83014, 0.10101]   \n",
       "4163  0.912675  0.417271   [0.07919, 0.22922, 0.63526, 0.80392, 0.1026]   \n",
       "2659  0.918037  0.402143  [0.07795, 0.20388, 0.69039, 0.79138, 0.10307]   \n",
       "4211  0.920030  0.415380    [0.07743, 0.22654, 0.67213, 0.79472, 0.101]   \n",
       "\n",
       "                                             recall  \\\n",
       "5587   [0.27049, 0.25373, 0.37795, 0.48093, 0.4586]   \n",
       "2627   [0.27049, 0.31343, 0.3867, 0.46328, 0.49682]   \n",
       "5751   [0.30328, 0.26567, 0.3762, 0.48729, 0.43949]   \n",
       "5735    [0.2459, 0.29254, 0.36045, 0.4887, 0.52866]   \n",
       "4115   [0.31148, 0.25373, 0.3797, 0.46257, 0.55414]   \n",
       "2691  [0.31148, 0.30149, 0.37358, 0.49223, 0.53503]   \n",
       "4067  [0.27869, 0.35821, 0.37708, 0.45904, 0.50955]   \n",
       "4163   [0.28689, 0.27164, 0.3657, 0.49223, 0.52866]   \n",
       "2659  [0.33607, 0.25075, 0.33946, 0.47952, 0.53503]   \n",
       "4211  [0.28689, 0.29552, 0.35871, 0.48941, 0.51592]   \n",
       "\n",
       "                                           f1_score  \\\n",
       "5587   [0.11018, 0.23578, 0.47814, 0.60106, 0.1511]   \n",
       "2627   [0.12088, 0.2518, 0.47374, 0.58729, 0.18014]   \n",
       "5751  [0.12151, 0.25035, 0.46714, 0.60131, 0.15506]   \n",
       "5735    [0.11194, 0.25554, 0.4637, 0.6105, 0.16617]   \n",
       "4115  [0.12102, 0.24745, 0.48009, 0.59383, 0.17109]   \n",
       "2691   [0.13451, 0.24908, 0.4814, 0.61167, 0.18321]   \n",
       "4067   [0.12928, 0.28169, 0.47363, 0.59118, 0.1686]   \n",
       "4163   [0.12411, 0.24863, 0.46419, 0.6106, 0.17184]   \n",
       "2659   [0.12654, 0.2249, 0.45513, 0.59719, 0.17284]   \n",
       "4211  [0.12195, 0.25648, 0.46777, 0.60577, 0.16893]   \n",
       "\n",
       "                          support  \n",
       "5587  [122, 335, 1143, 1416, 157]  \n",
       "2627  [122, 335, 1143, 1416, 157]  \n",
       "5751  [122, 335, 1143, 1416, 157]  \n",
       "5735  [122, 335, 1143, 1416, 157]  \n",
       "4115  [122, 335, 1143, 1416, 157]  \n",
       "2691  [122, 335, 1143, 1416, 157]  \n",
       "4067  [122, 335, 1143, 1416, 157]  \n",
       "4163  [122, 335, 1143, 1416, 157]  \n",
       "2659  [122, 335, 1143, 1416, 157]  \n",
       "4211  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_first_ft.sort_values(by=['loss']).iloc[:10, -6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the corresponding hyperparameters, we understand that the best performance are obtained with hidden sizes greater than 16: therefore we extend the space of hidden sizes values. Moreover, we want to make experiment with greater value of batch_size (512) and a lesser learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining second hyperparameters space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [3, 5, 7, 10],\n",
    "\t'hidden_size' : [16, 64, 128, 256],\n",
    "\t'batch_size' : [16, 64, 256, 512],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None, nn.LogSoftmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')], \n",
    "\t'dropout' : [0, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [0.01, 1e-5], \n",
    "\t'optimizer': [\"torch.optim.SGD\", \"torch.optim.Adam\"],\n",
    "\t'weight_decay': [1e-4]\t\t\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better readability, we do not present once again the code implementing the training and the testing.\n",
    "\n",
    "Within this second fine tuning, we enhance the performance analysis by computing also the sums of precision, recall and f1_score which are conditional to single classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_second_ft.to_csv(\"tuning_hyperparams/results_second_ft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_second_ft = pd.read_csv(\"tuning_hyperparams/results_second_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in descending order by accuracy: we note null precisions and recalls regarding class with lower frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.316624</td>\n",
       "      <td>0.448471</td>\n",
       "      <td>[0.11633, 0.20382, 0.48902, 0.93333, 0.15126]</td>\n",
       "      <td>1.893765</td>\n",
       "      <td>[0.46721, 0.38209, 0.56518, 0.40537, 0.11465]</td>\n",
       "      <td>1.934499</td>\n",
       "      <td>[0.18627, 0.26584, 0.52435, 0.56524, 0.13043]</td>\n",
       "      <td>1.672135</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.611139</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.626771</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.609784</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.611960</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.612005</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.612719</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.610725</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.623199</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.611939</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                                      precision  \\\n",
       "41   1.316624  0.448471  [0.11633, 0.20382, 0.48902, 0.93333, 0.15126]   \n",
       "56   1.611139  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "9    1.626771  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "101  1.609784  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "104  1.611960  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "48   1.612005  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "29   1.612719  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "53   1.610725  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "12   1.623199  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "64   1.611939  0.446265                  [0.0, 0.0, 0.0, 0.44627, 0.0]   \n",
       "\n",
       "     precision_total                                         recall  \\\n",
       "41          1.893765  [0.46721, 0.38209, 0.56518, 0.40537, 0.11465]   \n",
       "56          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "9           0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "101         0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "104         0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "48          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "29          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "53          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "12          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "64          0.446265                      [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
       "\n",
       "     recall_total                                       f1_score  \\\n",
       "41       1.934499  [0.18627, 0.26584, 0.52435, 0.56524, 0.13043]   \n",
       "56       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "9        1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "101      1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "104      1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "48       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "29       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "53       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "12       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "64       1.000000                  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "41         1.672135  [122, 335, 1143, 1416, 157]  \n",
       "56         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "9          0.617128  [122, 335, 1143, 1416, 157]  \n",
       "101        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "104        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "48         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "29         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "53         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "12         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "64         0.617128  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_second_ft.sort_values(by=['accuracy'], ascending=False).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in ascending order by loss: the precision and recall regarding class with lower frequency are still quite imbalanced with respect to class with higher frequency but to a lesser extent. However, the second fine tuning generally leads to a lower accuracy and a higher loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.080970</td>\n",
       "      <td>0.386070</td>\n",
       "      <td>[0.08269, 0.22753, 0.64634, 0.80328, 0.1]</td>\n",
       "      <td>1.859841</td>\n",
       "      <td>[0.35246, 0.24179, 0.32458, 0.44986, 0.59236]</td>\n",
       "      <td>1.961050</td>\n",
       "      <td>[0.13396, 0.23444, 0.43215, 0.57673, 0.17111]</td>\n",
       "      <td>1.548393</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.098394</td>\n",
       "      <td>0.381658</td>\n",
       "      <td>[0.08611, 0.22701, 0.63393, 0.83508, 0.09596]</td>\n",
       "      <td>1.878084</td>\n",
       "      <td>[0.36066, 0.23582, 0.31059, 0.45056, 0.6051]</td>\n",
       "      <td>1.962723</td>\n",
       "      <td>[0.13902, 0.23133, 0.41691, 0.58532, 0.16565]</td>\n",
       "      <td>1.538235</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.103853</td>\n",
       "      <td>0.377246</td>\n",
       "      <td>[0.08961, 0.2125, 0.63194, 0.8099, 0.09779]</td>\n",
       "      <td>1.841738</td>\n",
       "      <td>[0.40984, 0.20299, 0.31846, 0.43927, 0.59236]</td>\n",
       "      <td>1.962904</td>\n",
       "      <td>[0.14706, 0.20763, 0.4235, 0.5696, 0.16787]</td>\n",
       "      <td>1.515662</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.155672</td>\n",
       "      <td>0.367160</td>\n",
       "      <td>[0.08722, 0.23214, 0.6568, 0.84637, 0.09555]</td>\n",
       "      <td>1.918089</td>\n",
       "      <td>[0.35246, 0.27164, 0.29134, 0.4202, 0.65605]</td>\n",
       "      <td>1.991688</td>\n",
       "      <td>[0.13984, 0.25034, 0.40364, 0.56159, 0.1668]</td>\n",
       "      <td>1.522205</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.160277</td>\n",
       "      <td>0.364324</td>\n",
       "      <td>[0.0775, 0.23843, 0.64272, 0.8209, 0.09389]</td>\n",
       "      <td>1.873449</td>\n",
       "      <td>[0.33607, 0.2, 0.29746, 0.42726, 0.65605]</td>\n",
       "      <td>1.916839</td>\n",
       "      <td>[0.12596, 0.21753, 0.4067, 0.56201, 0.16427]</td>\n",
       "      <td>1.476472</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.160518</td>\n",
       "      <td>0.364324</td>\n",
       "      <td>[0.07356, 0.2153, 0.65577, 0.80345, 0.09291]</td>\n",
       "      <td>1.840990</td>\n",
       "      <td>[0.30328, 0.22687, 0.29834, 0.42726, 0.61783]</td>\n",
       "      <td>1.873576</td>\n",
       "      <td>[0.1184, 0.22093, 0.4101, 0.55786, 0.16153]</td>\n",
       "      <td>1.468825</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.163139</td>\n",
       "      <td>0.375355</td>\n",
       "      <td>[0.09091, 0.24501, 0.68952, 0.84203, 0.09242]</td>\n",
       "      <td>1.959900</td>\n",
       "      <td>[0.37705, 0.25672, 0.31671, 0.42161, 0.63694]</td>\n",
       "      <td>2.009029</td>\n",
       "      <td>[0.1465, 0.25073, 0.43405, 0.56188, 0.16142]</td>\n",
       "      <td>1.554581</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.165624</td>\n",
       "      <td>0.365585</td>\n",
       "      <td>[0.09091, 0.2112, 0.64245, 0.82639, 0.09657]</td>\n",
       "      <td>1.867512</td>\n",
       "      <td>[0.31967, 0.24776, 0.29396, 0.4202, 0.68153]</td>\n",
       "      <td>1.963123</td>\n",
       "      <td>[0.14156, 0.22802, 0.40336, 0.55712, 0.16917]</td>\n",
       "      <td>1.499230</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.251483</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>[0.08277, 0.25455, 0.64646, 0.83843, 0.09342]</td>\n",
       "      <td>1.915625</td>\n",
       "      <td>[0.40164, 0.20896, 0.27997, 0.40678, 0.66879]</td>\n",
       "      <td>1.966129</td>\n",
       "      <td>[0.13725, 0.22951, 0.39072, 0.54779, 0.16393]</td>\n",
       "      <td>1.469207</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.277507</td>\n",
       "      <td>0.363694</td>\n",
       "      <td>[0.08945, 0.31383, 0.78864, 0.93596, 0.09351]</td>\n",
       "      <td>2.221392</td>\n",
       "      <td>[0.54918, 0.17612, 0.30359, 0.40254, 0.70701]</td>\n",
       "      <td>2.138436</td>\n",
       "      <td>[0.15385, 0.22562, 0.43841, 0.56296, 0.16518]</td>\n",
       "      <td>1.546017</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                                      precision  \\\n",
       "91   1.080970  0.386070      [0.08269, 0.22753, 0.64634, 0.80328, 0.1]   \n",
       "83   1.098394  0.381658  [0.08611, 0.22701, 0.63393, 0.83508, 0.09596]   \n",
       "107  1.103853  0.377246    [0.08961, 0.2125, 0.63194, 0.8099, 0.09779]   \n",
       "75   1.155672  0.367160   [0.08722, 0.23214, 0.6568, 0.84637, 0.09555]   \n",
       "59   1.160277  0.364324    [0.0775, 0.23843, 0.64272, 0.8209, 0.09389]   \n",
       "99   1.160518  0.364324   [0.07356, 0.2153, 0.65577, 0.80345, 0.09291]   \n",
       "51   1.163139  0.375355  [0.09091, 0.24501, 0.68952, 0.84203, 0.09242]   \n",
       "67   1.165624  0.365585   [0.09091, 0.2112, 0.64245, 0.82639, 0.09657]   \n",
       "19   1.251483  0.352978  [0.08277, 0.25455, 0.64646, 0.83843, 0.09342]   \n",
       "57   1.277507  0.363694  [0.08945, 0.31383, 0.78864, 0.93596, 0.09351]   \n",
       "\n",
       "     precision_total                                         recall  \\\n",
       "91          1.859841  [0.35246, 0.24179, 0.32458, 0.44986, 0.59236]   \n",
       "83          1.878084   [0.36066, 0.23582, 0.31059, 0.45056, 0.6051]   \n",
       "107         1.841738  [0.40984, 0.20299, 0.31846, 0.43927, 0.59236]   \n",
       "75          1.918089   [0.35246, 0.27164, 0.29134, 0.4202, 0.65605]   \n",
       "59          1.873449      [0.33607, 0.2, 0.29746, 0.42726, 0.65605]   \n",
       "99          1.840990  [0.30328, 0.22687, 0.29834, 0.42726, 0.61783]   \n",
       "51          1.959900  [0.37705, 0.25672, 0.31671, 0.42161, 0.63694]   \n",
       "67          1.867512   [0.31967, 0.24776, 0.29396, 0.4202, 0.68153]   \n",
       "19          1.915625  [0.40164, 0.20896, 0.27997, 0.40678, 0.66879]   \n",
       "57          2.221392  [0.54918, 0.17612, 0.30359, 0.40254, 0.70701]   \n",
       "\n",
       "     recall_total                                       f1_score  \\\n",
       "91       1.961050  [0.13396, 0.23444, 0.43215, 0.57673, 0.17111]   \n",
       "83       1.962723  [0.13902, 0.23133, 0.41691, 0.58532, 0.16565]   \n",
       "107      1.962904    [0.14706, 0.20763, 0.4235, 0.5696, 0.16787]   \n",
       "75       1.991688   [0.13984, 0.25034, 0.40364, 0.56159, 0.1668]   \n",
       "59       1.916839   [0.12596, 0.21753, 0.4067, 0.56201, 0.16427]   \n",
       "99       1.873576    [0.1184, 0.22093, 0.4101, 0.55786, 0.16153]   \n",
       "51       2.009029   [0.1465, 0.25073, 0.43405, 0.56188, 0.16142]   \n",
       "67       1.963123  [0.14156, 0.22802, 0.40336, 0.55712, 0.16917]   \n",
       "19       1.966129  [0.13725, 0.22951, 0.39072, 0.54779, 0.16393]   \n",
       "57       2.138436  [0.15385, 0.22562, 0.43841, 0.56296, 0.16518]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "91         1.548393  [122, 335, 1143, 1416, 157]  \n",
       "83         1.538235  [122, 335, 1143, 1416, 157]  \n",
       "107        1.515662  [122, 335, 1143, 1416, 157]  \n",
       "75         1.522205  [122, 335, 1143, 1416, 157]  \n",
       "59         1.476472  [122, 335, 1143, 1416, 157]  \n",
       "99         1.468825  [122, 335, 1143, 1416, 157]  \n",
       "51         1.554581  [122, 335, 1143, 1416, 157]  \n",
       "67         1.499230  [122, 335, 1143, 1416, 157]  \n",
       "19         1.469207  [122, 335, 1143, 1416, 157]  \n",
       "57         1.546017  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_second_ft.sort_values(by=['loss']).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in descending order by f1_score, which synthesize the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.316624</td>\n",
       "      <td>0.448471</td>\n",
       "      <td>[0.11633, 0.20382, 0.48902, 0.93333, 0.15126]</td>\n",
       "      <td>1.893765</td>\n",
       "      <td>[0.46721, 0.38209, 0.56518, 0.40537, 0.11465]</td>\n",
       "      <td>1.934499</td>\n",
       "      <td>[0.18627, 0.26584, 0.52435, 0.56524, 0.13043]</td>\n",
       "      <td>1.672135</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.291049</td>\n",
       "      <td>0.379136</td>\n",
       "      <td>[0.08971, 0.34146, 0.72505, 0.94617, 0.09413]</td>\n",
       "      <td>2.196519</td>\n",
       "      <td>[0.61475, 0.20896, 0.33683, 0.4096, 0.59236]</td>\n",
       "      <td>2.162503</td>\n",
       "      <td>[0.15658, 0.25926, 0.45998, 0.57171, 0.16245]</td>\n",
       "      <td>1.609967</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.298850</td>\n",
       "      <td>0.364639</td>\n",
       "      <td>[0.09821, 0.36111, 0.78005, 0.9479, 0.09183]</td>\n",
       "      <td>2.279099</td>\n",
       "      <td>[0.54098, 0.19403, 0.30096, 0.39831, 0.75159]</td>\n",
       "      <td>2.185873</td>\n",
       "      <td>[0.16625, 0.25243, 0.43434, 0.56091, 0.16366]</td>\n",
       "      <td>1.577594</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.163139</td>\n",
       "      <td>0.375355</td>\n",
       "      <td>[0.09091, 0.24501, 0.68952, 0.84203, 0.09242]</td>\n",
       "      <td>1.959900</td>\n",
       "      <td>[0.37705, 0.25672, 0.31671, 0.42161, 0.63694]</td>\n",
       "      <td>2.009029</td>\n",
       "      <td>[0.1465, 0.25073, 0.43405, 0.56188, 0.16142]</td>\n",
       "      <td>1.554581</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.080970</td>\n",
       "      <td>0.386070</td>\n",
       "      <td>[0.08269, 0.22753, 0.64634, 0.80328, 0.1]</td>\n",
       "      <td>1.859841</td>\n",
       "      <td>[0.35246, 0.24179, 0.32458, 0.44986, 0.59236]</td>\n",
       "      <td>1.961050</td>\n",
       "      <td>[0.13396, 0.23444, 0.43215, 0.57673, 0.17111]</td>\n",
       "      <td>1.548393</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.277507</td>\n",
       "      <td>0.363694</td>\n",
       "      <td>[0.08945, 0.31383, 0.78864, 0.93596, 0.09351]</td>\n",
       "      <td>2.221392</td>\n",
       "      <td>[0.54918, 0.17612, 0.30359, 0.40254, 0.70701]</td>\n",
       "      <td>2.138436</td>\n",
       "      <td>[0.15385, 0.22562, 0.43841, 0.56296, 0.16518]</td>\n",
       "      <td>1.546017</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.285883</td>\n",
       "      <td>0.355184</td>\n",
       "      <td>[0.10094, 0.70312, 0.80535, 0.94359, 0.09051]</td>\n",
       "      <td>2.643521</td>\n",
       "      <td>[0.61475, 0.13433, 0.28959, 0.38983, 0.78981]</td>\n",
       "      <td>2.218311</td>\n",
       "      <td>[0.17341, 0.22556, 0.426, 0.55172, 0.16241]</td>\n",
       "      <td>1.539106</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.098394</td>\n",
       "      <td>0.381658</td>\n",
       "      <td>[0.08611, 0.22701, 0.63393, 0.83508, 0.09596]</td>\n",
       "      <td>1.878084</td>\n",
       "      <td>[0.36066, 0.23582, 0.31059, 0.45056, 0.6051]</td>\n",
       "      <td>1.962723</td>\n",
       "      <td>[0.13902, 0.23133, 0.41691, 0.58532, 0.16565]</td>\n",
       "      <td>1.538235</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.290532</td>\n",
       "      <td>0.354554</td>\n",
       "      <td>[0.09446, 0.42424, 0.79268, 0.94898, 0.08887]</td>\n",
       "      <td>2.349234</td>\n",
       "      <td>[0.61475, 0.16716, 0.28434, 0.39407, 0.70701]</td>\n",
       "      <td>2.167332</td>\n",
       "      <td>[0.16376, 0.23983, 0.41854, 0.55689, 0.15789]</td>\n",
       "      <td>1.536910</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.280429</td>\n",
       "      <td>0.360227</td>\n",
       "      <td>[0.09852, 0.75472, 0.80048, 0.94676, 0.09138]</td>\n",
       "      <td>2.691841</td>\n",
       "      <td>[0.59836, 0.1194, 0.29484, 0.40184, 0.78981]</td>\n",
       "      <td>2.204247</td>\n",
       "      <td>[0.16918, 0.20619, 0.43095, 0.5642, 0.1638]</td>\n",
       "      <td>1.534318</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                                      precision  \\\n",
       "41   1.316624  0.448471  [0.11633, 0.20382, 0.48902, 0.93333, 0.15126]   \n",
       "105  1.291049  0.379136  [0.08971, 0.34146, 0.72505, 0.94617, 0.09413]   \n",
       "73   1.298850  0.364639   [0.09821, 0.36111, 0.78005, 0.9479, 0.09183]   \n",
       "51   1.163139  0.375355  [0.09091, 0.24501, 0.68952, 0.84203, 0.09242]   \n",
       "91   1.080970  0.386070      [0.08269, 0.22753, 0.64634, 0.80328, 0.1]   \n",
       "57   1.277507  0.363694  [0.08945, 0.31383, 0.78864, 0.93596, 0.09351]   \n",
       "89   1.285883  0.355184  [0.10094, 0.70312, 0.80535, 0.94359, 0.09051]   \n",
       "83   1.098394  0.381658  [0.08611, 0.22701, 0.63393, 0.83508, 0.09596]   \n",
       "97   1.290532  0.354554  [0.09446, 0.42424, 0.79268, 0.94898, 0.08887]   \n",
       "81   1.280429  0.360227  [0.09852, 0.75472, 0.80048, 0.94676, 0.09138]   \n",
       "\n",
       "     precision_total                                         recall  \\\n",
       "41          1.893765  [0.46721, 0.38209, 0.56518, 0.40537, 0.11465]   \n",
       "105         2.196519   [0.61475, 0.20896, 0.33683, 0.4096, 0.59236]   \n",
       "73          2.279099  [0.54098, 0.19403, 0.30096, 0.39831, 0.75159]   \n",
       "51          1.959900  [0.37705, 0.25672, 0.31671, 0.42161, 0.63694]   \n",
       "91          1.859841  [0.35246, 0.24179, 0.32458, 0.44986, 0.59236]   \n",
       "57          2.221392  [0.54918, 0.17612, 0.30359, 0.40254, 0.70701]   \n",
       "89          2.643521  [0.61475, 0.13433, 0.28959, 0.38983, 0.78981]   \n",
       "83          1.878084   [0.36066, 0.23582, 0.31059, 0.45056, 0.6051]   \n",
       "97          2.349234  [0.61475, 0.16716, 0.28434, 0.39407, 0.70701]   \n",
       "81          2.691841   [0.59836, 0.1194, 0.29484, 0.40184, 0.78981]   \n",
       "\n",
       "     recall_total                                       f1_score  \\\n",
       "41       1.934499  [0.18627, 0.26584, 0.52435, 0.56524, 0.13043]   \n",
       "105      2.162503  [0.15658, 0.25926, 0.45998, 0.57171, 0.16245]   \n",
       "73       2.185873  [0.16625, 0.25243, 0.43434, 0.56091, 0.16366]   \n",
       "51       2.009029   [0.1465, 0.25073, 0.43405, 0.56188, 0.16142]   \n",
       "91       1.961050  [0.13396, 0.23444, 0.43215, 0.57673, 0.17111]   \n",
       "57       2.138436  [0.15385, 0.22562, 0.43841, 0.56296, 0.16518]   \n",
       "89       2.218311    [0.17341, 0.22556, 0.426, 0.55172, 0.16241]   \n",
       "83       1.962723  [0.13902, 0.23133, 0.41691, 0.58532, 0.16565]   \n",
       "97       2.167332  [0.16376, 0.23983, 0.41854, 0.55689, 0.15789]   \n",
       "81       2.204247    [0.16918, 0.20619, 0.43095, 0.5642, 0.1638]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "41         1.672135  [122, 335, 1143, 1416, 157]  \n",
       "105        1.609967  [122, 335, 1143, 1416, 157]  \n",
       "73         1.577594  [122, 335, 1143, 1416, 157]  \n",
       "51         1.554581  [122, 335, 1143, 1416, 157]  \n",
       "91         1.548393  [122, 335, 1143, 1416, 157]  \n",
       "57         1.546017  [122, 335, 1143, 1416, 157]  \n",
       "89         1.539106  [122, 335, 1143, 1416, 157]  \n",
       "83         1.538235  [122, 335, 1143, 1416, 157]  \n",
       "97         1.536910  [122, 335, 1143, 1416, 157]  \n",
       "81         1.534318  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_second_ft.sort_values(by=['f1_score_total'], ascending=False).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first trainings seems to be a good candidate as the best hyperparameters' configuration, since it presents an acceptable accuracy (0.44, given the fact that a random classifier over 5 classes presents an accuracy equal to 0.2) and low loss (1.31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nr_train                                                      5803\n",
       "num_epochs                                                     500\n",
       "n_bad_epochs                                                     3\n",
       "num_hidden_layers                                               10\n",
       "hidden_size                                                     64\n",
       "batch_size                                                     512\n",
       "af_first_layer                      LeakyReLU(negative_slope=0.01)\n",
       "af_hidden_layers                    LeakyReLU(negative_slope=0.01)\n",
       "af_output_layer                                  LogSoftmax(dim=1)\n",
       "loss_function                                          KLDivLoss()\n",
       "dropout                                                        0.0\n",
       "batch_norm                                                   False\n",
       "learning_rate                                              0.00001\n",
       "optimizer                                         torch.optim.Adam\n",
       "weight_decay                                                0.0001\n",
       "epoch_stopped                                                   66\n",
       "loss                                                      1.316624\n",
       "accuracy                                                  0.448471\n",
       "precision            [0.11633, 0.20382, 0.48902, 0.93333, 0.15126]\n",
       "precision_total                                           1.893765\n",
       "recall               [0.46721, 0.38209, 0.56518, 0.40537, 0.11465]\n",
       "recall_total                                              1.934499\n",
       "f1_score             [0.18627, 0.26584, 0.52435, 0.56524, 0.13043]\n",
       "f1_score_total                                            1.672135\n",
       "support                                [122, 335, 1143, 1416, 157]\n",
       "Name: 41, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_second_ft.sort_values(by=['f1_score_total'], ascending=False).iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining third hyperparameters space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the corresponding hyperparameters, we understand that the best performance are obtained with higher number of hidden layers and bigger batches, so we extend their space: moreover, we delete 0.01 as learning rate and 0.2 as dropout probability since they do not lead to good performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [3, 5, 7, 10],\n",
    "\t'hidden_size' : [16, 64, 128, 256],\n",
    "\t'batch_size' : [16, 64, 256, 512],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None, nn.LogSoftmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')], \n",
    "\t'dropout' : [0, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [0.01, 1e-5], \n",
    "\t'optimizer': [\"torch.optim.SGD\", \"torch.optim.Adam\"],\n",
    "\t'weight_decay': [1e-4]\t\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [12, 15, 18],\n",
    "\t'hidden_size' : [64, 128, 256],\n",
    "\t'batch_size' : [256, 512, 1024, 2048],\n",
    "\t'af_first_layer' : [nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None, nn.LogSoftmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')], \n",
    "\t'dropout' : [0, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [1e-5], \n",
    "\t'optimizer': [\"torch.optim.Adam\"],\n",
    "\t'weight_decay': [1e-4]\t\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_third_ft.to_csv(\"tuning_hyperparams/results_third_ft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_third_ft = pd.read_csv(\"tuning_hyperparams/results_third_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in descending order by accuracy: we note null precisions and recalls regarding class with lower frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.610409</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1.609764</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1.609740</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.613400</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.609661</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1.609421</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.611898</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1.609957</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.609902</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1.610333</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.44627, 0.0]</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.61713, 0.0]</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                      precision  precision_total  \\\n",
       "0    1.610409  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "244  1.609764  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "240  1.609740  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "28   1.613400  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "262  1.609661  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "264  1.609421  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "56   1.611898  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "232  1.609957  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "60   1.609902  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "230  1.610333  0.446265  [0.0, 0.0, 0.0, 0.44627, 0.0]         0.446265   \n",
       "\n",
       "                        recall  recall_total                       f1_score  \\\n",
       "0    [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "244  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "240  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "28   [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "262  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "264  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "56   [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "232  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "60   [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "230  [0.0, 0.0, 0.0, 1.0, 0.0]           1.0  [0.0, 0.0, 0.0, 0.61713, 0.0]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "0          0.617128  [122, 335, 1143, 1416, 157]  \n",
       "244        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "240        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "28         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "262        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "264        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "56         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "232        0.617128  [122, 335, 1143, 1416, 157]  \n",
       "60         0.617128  [122, 335, 1143, 1416, 157]  \n",
       "230        0.617128  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_third_ft.sort_values(by=['accuracy'], ascending=False).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in ascending order by loss: the precision and recall regarding class with lower frequency are still quite imbalanced with respect to class with higher frequency but to a lesser extent. However, the second fine tuning generally leads to a lower accuracy and a higher loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.063103</td>\n",
       "      <td>0.345730</td>\n",
       "      <td>[0.08015, 0.20163, 0.55172, 0.73881, 0.0898]</td>\n",
       "      <td>1.662111</td>\n",
       "      <td>[0.36066, 0.2209, 0.26597, 0.41949, 0.51592]</td>\n",
       "      <td>1.782933</td>\n",
       "      <td>[0.13115, 0.21083, 0.35891, 0.53514, 0.15297]</td>\n",
       "      <td>1.388997</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.070294</td>\n",
       "      <td>0.340372</td>\n",
       "      <td>[0.0748, 0.17344, 0.54919, 0.72795, 0.0912]</td>\n",
       "      <td>1.616592</td>\n",
       "      <td>[0.31148, 0.19104, 0.26859, 0.41384, 0.5414]</td>\n",
       "      <td>1.726355</td>\n",
       "      <td>[0.12063, 0.18182, 0.36075, 0.52769, 0.15611]</td>\n",
       "      <td>1.347002</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1.074537</td>\n",
       "      <td>0.328396</td>\n",
       "      <td>[0.08368, 0.1687, 0.50333, 0.72118, 0.09894]</td>\n",
       "      <td>1.575835</td>\n",
       "      <td>[0.32787, 0.20597, 0.26422, 0.37994, 0.59236]</td>\n",
       "      <td>1.770356</td>\n",
       "      <td>[0.13333, 0.18548, 0.34653, 0.49769, 0.16955]</td>\n",
       "      <td>1.332587</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.079831</td>\n",
       "      <td>0.371257</td>\n",
       "      <td>[0.08268, 0.22781, 0.62101, 0.78315, 0.09726]</td>\n",
       "      <td>1.811913</td>\n",
       "      <td>[0.34426, 0.22985, 0.28959, 0.44633, 0.61146]</td>\n",
       "      <td>1.921494</td>\n",
       "      <td>[0.13333, 0.22883, 0.39499, 0.5686, 0.16783]</td>\n",
       "      <td>1.493581</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.094534</td>\n",
       "      <td>0.344469</td>\n",
       "      <td>[0.07773, 0.19841, 0.56228, 0.78219, 0.09153]</td>\n",
       "      <td>1.712142</td>\n",
       "      <td>[0.30328, 0.22388, 0.27647, 0.40325, 0.59873]</td>\n",
       "      <td>1.805599</td>\n",
       "      <td>[0.12375, 0.21038, 0.37067, 0.53215, 0.15878]</td>\n",
       "      <td>1.395736</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.098131</td>\n",
       "      <td>0.356760</td>\n",
       "      <td>[0.07629, 0.18378, 0.57841, 0.80518, 0.09384]</td>\n",
       "      <td>1.737509</td>\n",
       "      <td>[0.30328, 0.20299, 0.30009, 0.41737, 0.59236]</td>\n",
       "      <td>1.816081</td>\n",
       "      <td>[0.12191, 0.19291, 0.39516, 0.54977, 0.16202]</td>\n",
       "      <td>1.421768</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.103233</td>\n",
       "      <td>0.357706</td>\n",
       "      <td>[0.08299, 0.21902, 0.60256, 0.78129, 0.09336]</td>\n",
       "      <td>1.779222</td>\n",
       "      <td>[0.32787, 0.22687, 0.28784, 0.41879, 0.61783]</td>\n",
       "      <td>1.879193</td>\n",
       "      <td>[0.13245, 0.22287, 0.38958, 0.54529, 0.16221]</td>\n",
       "      <td>1.452399</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.104342</td>\n",
       "      <td>0.326505</td>\n",
       "      <td>[0.07963, 0.16316, 0.51646, 0.73978, 0.09554]</td>\n",
       "      <td>1.594575</td>\n",
       "      <td>[0.35246, 0.18507, 0.26072, 0.38347, 0.57325]</td>\n",
       "      <td>1.754974</td>\n",
       "      <td>[0.12991, 0.17343, 0.34651, 0.50512, 0.16379]</td>\n",
       "      <td>1.318749</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.106367</td>\n",
       "      <td>0.364639</td>\n",
       "      <td>[0.08408, 0.22164, 0.64466, 0.82434, 0.09829]</td>\n",
       "      <td>1.873013</td>\n",
       "      <td>[0.38525, 0.25075, 0.29046, 0.4209, 0.6242]</td>\n",
       "      <td>1.971564</td>\n",
       "      <td>[0.13803, 0.23529, 0.40048, 0.55727, 0.16984]</td>\n",
       "      <td>1.500923</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.107266</td>\n",
       "      <td>0.348566</td>\n",
       "      <td>[0.08485, 0.21833, 0.57268, 0.77217, 0.08973]</td>\n",
       "      <td>1.737757</td>\n",
       "      <td>[0.34426, 0.24179, 0.28609, 0.39972, 0.57325]</td>\n",
       "      <td>1.845109</td>\n",
       "      <td>[0.13614, 0.22946, 0.38156, 0.52676, 0.15517]</td>\n",
       "      <td>1.429097</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                                      precision  \\\n",
       "189  1.063103  0.345730   [0.08015, 0.20163, 0.55172, 0.73881, 0.0898]   \n",
       "177  1.070294  0.340372    [0.0748, 0.17344, 0.54919, 0.72795, 0.0912]   \n",
       "285  1.074537  0.328396   [0.08368, 0.1687, 0.50333, 0.72118, 0.09894]   \n",
       "77   1.079831  0.371257  [0.08268, 0.22781, 0.62101, 0.78315, 0.09726]   \n",
       "93   1.094534  0.344469  [0.07773, 0.19841, 0.56228, 0.78219, 0.09153]   \n",
       "85   1.098131  0.356760  [0.07629, 0.18378, 0.57841, 0.80518, 0.09384]   \n",
       "73   1.103233  0.357706  [0.08299, 0.21902, 0.60256, 0.78129, 0.09336]   \n",
       "273  1.104342  0.326505  [0.07963, 0.16316, 0.51646, 0.73978, 0.09554]   \n",
       "165  1.106367  0.364639  [0.08408, 0.22164, 0.64466, 0.82434, 0.09829]   \n",
       "81   1.107266  0.348566  [0.08485, 0.21833, 0.57268, 0.77217, 0.08973]   \n",
       "\n",
       "     precision_total                                         recall  \\\n",
       "189         1.662111   [0.36066, 0.2209, 0.26597, 0.41949, 0.51592]   \n",
       "177         1.616592   [0.31148, 0.19104, 0.26859, 0.41384, 0.5414]   \n",
       "285         1.575835  [0.32787, 0.20597, 0.26422, 0.37994, 0.59236]   \n",
       "77          1.811913  [0.34426, 0.22985, 0.28959, 0.44633, 0.61146]   \n",
       "93          1.712142  [0.30328, 0.22388, 0.27647, 0.40325, 0.59873]   \n",
       "85          1.737509  [0.30328, 0.20299, 0.30009, 0.41737, 0.59236]   \n",
       "73          1.779222  [0.32787, 0.22687, 0.28784, 0.41879, 0.61783]   \n",
       "273         1.594575  [0.35246, 0.18507, 0.26072, 0.38347, 0.57325]   \n",
       "165         1.873013    [0.38525, 0.25075, 0.29046, 0.4209, 0.6242]   \n",
       "81          1.737757  [0.34426, 0.24179, 0.28609, 0.39972, 0.57325]   \n",
       "\n",
       "     recall_total                                       f1_score  \\\n",
       "189      1.782933  [0.13115, 0.21083, 0.35891, 0.53514, 0.15297]   \n",
       "177      1.726355  [0.12063, 0.18182, 0.36075, 0.52769, 0.15611]   \n",
       "285      1.770356  [0.13333, 0.18548, 0.34653, 0.49769, 0.16955]   \n",
       "77       1.921494   [0.13333, 0.22883, 0.39499, 0.5686, 0.16783]   \n",
       "93       1.805599  [0.12375, 0.21038, 0.37067, 0.53215, 0.15878]   \n",
       "85       1.816081  [0.12191, 0.19291, 0.39516, 0.54977, 0.16202]   \n",
       "73       1.879193  [0.13245, 0.22287, 0.38958, 0.54529, 0.16221]   \n",
       "273      1.754974  [0.12991, 0.17343, 0.34651, 0.50512, 0.16379]   \n",
       "165      1.971564  [0.13803, 0.23529, 0.40048, 0.55727, 0.16984]   \n",
       "81       1.845109  [0.13614, 0.22946, 0.38156, 0.52676, 0.15517]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "189        1.388997  [122, 335, 1143, 1416, 157]  \n",
       "177        1.347002  [122, 335, 1143, 1416, 157]  \n",
       "285        1.332587  [122, 335, 1143, 1416, 157]  \n",
       "77         1.493581  [122, 335, 1143, 1416, 157]  \n",
       "93         1.395736  [122, 335, 1143, 1416, 157]  \n",
       "85         1.421768  [122, 335, 1143, 1416, 157]  \n",
       "73         1.452399  [122, 335, 1143, 1416, 157]  \n",
       "273        1.318749  [122, 335, 1143, 1416, 157]  \n",
       "165        1.500923  [122, 335, 1143, 1416, 157]  \n",
       "81         1.429097  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_third_ft.sort_values(by=['loss']).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the first 10 trainings sorted in descending order by f1_score, however the first ones present accuracies lower than that of the beforementioned good configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>precision_total</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall_total</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score_total</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.283165</td>\n",
       "      <td>0.376930</td>\n",
       "      <td>[0.13428, 0.22107, 0.78251, 0.93677, 0.09335]</td>\n",
       "      <td>2.167984</td>\n",
       "      <td>[0.31148, 0.38209, 0.30534, 0.3976, 0.75159]</td>\n",
       "      <td>2.148093</td>\n",
       "      <td>[0.18765, 0.28009, 0.43927, 0.55825, 0.16608]</td>\n",
       "      <td>1.631347</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.303822</td>\n",
       "      <td>0.370942</td>\n",
       "      <td>[0.12058, 0.26269, 0.77427, 0.94676, 0.09063]</td>\n",
       "      <td>2.194923</td>\n",
       "      <td>[0.47541, 0.26269, 0.30009, 0.40184, 0.75796]</td>\n",
       "      <td>2.197982</td>\n",
       "      <td>[0.19237, 0.26269, 0.43253, 0.5642, 0.1619]</td>\n",
       "      <td>1.613702</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.295256</td>\n",
       "      <td>0.363063</td>\n",
       "      <td>[0.10375, 0.35417, 0.80145, 0.96055, 0.09373]</td>\n",
       "      <td>2.313641</td>\n",
       "      <td>[0.59016, 0.20299, 0.28959, 0.39548, 0.7707]</td>\n",
       "      <td>2.248919</td>\n",
       "      <td>[0.17647, 0.25806, 0.42545, 0.56028, 0.16713]</td>\n",
       "      <td>1.587392</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.254475</td>\n",
       "      <td>0.357075</td>\n",
       "      <td>[0.08079, 0.74667, 0.82915, 0.9527, 0.09144]</td>\n",
       "      <td>2.700744</td>\n",
       "      <td>[0.60656, 0.16716, 0.28871, 0.39831, 0.69427]</td>\n",
       "      <td>2.155008</td>\n",
       "      <td>[0.14258, 0.27317, 0.42829, 0.56175, 0.1616]</td>\n",
       "      <td>1.567400</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.286500</td>\n",
       "      <td>0.368736</td>\n",
       "      <td>[0.0922, 0.49485, 0.7584, 0.91733, 0.09422]</td>\n",
       "      <td>2.356999</td>\n",
       "      <td>[0.63934, 0.14328, 0.31584, 0.40749, 0.67516]</td>\n",
       "      <td>2.181108</td>\n",
       "      <td>[0.16116, 0.22222, 0.44595, 0.5643, 0.16537]</td>\n",
       "      <td>1.559003</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.265158</td>\n",
       "      <td>0.363694</td>\n",
       "      <td>[0.09439, 0.66154, 0.76068, 0.94684, 0.08852]</td>\n",
       "      <td>2.551971</td>\n",
       "      <td>[0.60656, 0.12836, 0.31146, 0.40254, 0.70701]</td>\n",
       "      <td>2.155925</td>\n",
       "      <td>[0.16336, 0.215, 0.44196, 0.56492, 0.15734]</td>\n",
       "      <td>1.542568</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.385441</td>\n",
       "      <td>0.363063</td>\n",
       "      <td>[0.07456, 0.30423, 0.74121, 0.83062, 0.0832]</td>\n",
       "      <td>2.033813</td>\n",
       "      <td>[0.27869, 0.32239, 0.25809, 0.43291, 0.64968]</td>\n",
       "      <td>1.941760</td>\n",
       "      <td>[0.11765, 0.31304, 0.38287, 0.56917, 0.14751]</td>\n",
       "      <td>1.530238</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.261881</td>\n",
       "      <td>0.354869</td>\n",
       "      <td>[0.09517, 0.44348, 0.83721, 0.94463, 0.08836]</td>\n",
       "      <td>2.408849</td>\n",
       "      <td>[0.53279, 0.15224, 0.28346, 0.3976, 0.78344]</td>\n",
       "      <td>2.149529</td>\n",
       "      <td>[0.16149, 0.22667, 0.42353, 0.55964, 0.15881]</td>\n",
       "      <td>1.530141</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.318057</td>\n",
       "      <td>0.350772</td>\n",
       "      <td>[0.09498, 0.72464, 0.78325, 0.92916, 0.08198]</td>\n",
       "      <td>2.614008</td>\n",
       "      <td>[0.57377, 0.14925, 0.27822, 0.39831, 0.70701]</td>\n",
       "      <td>2.106551</td>\n",
       "      <td>[0.16298, 0.24752, 0.41059, 0.55759, 0.14692]</td>\n",
       "      <td>1.525603</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.133167</td>\n",
       "      <td>0.366215</td>\n",
       "      <td>[0.08863, 0.21727, 0.66102, 0.82633, 0.09238]</td>\n",
       "      <td>1.885631</td>\n",
       "      <td>[0.37705, 0.23284, 0.30709, 0.41667, 0.61783]</td>\n",
       "      <td>1.951473</td>\n",
       "      <td>[0.14353, 0.22478, 0.41935, 0.55399, 0.16073]</td>\n",
       "      <td>1.502384</td>\n",
       "      <td>[122, 335, 1143, 1416, 157]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy                                      precision  \\\n",
       "76   1.283165  0.376930  [0.13428, 0.22107, 0.78251, 0.93677, 0.09335]   \n",
       "4    1.303822  0.370942  [0.12058, 0.26269, 0.77427, 0.94676, 0.09063]   \n",
       "36   1.295256  0.363063  [0.10375, 0.35417, 0.80145, 0.96055, 0.09373]   \n",
       "68   1.254475  0.357075   [0.08079, 0.74667, 0.82915, 0.9527, 0.09144]   \n",
       "72   1.286500  0.368736    [0.0922, 0.49485, 0.7584, 0.91733, 0.09422]   \n",
       "160  1.265158  0.363694  [0.09439, 0.66154, 0.76068, 0.94684, 0.08852]   \n",
       "100  1.385441  0.363063   [0.07456, 0.30423, 0.74121, 0.83062, 0.0832]   \n",
       "64   1.261881  0.354869  [0.09517, 0.44348, 0.83721, 0.94463, 0.08836]   \n",
       "32   1.318057  0.350772  [0.09498, 0.72464, 0.78325, 0.92916, 0.08198]   \n",
       "65   1.133167  0.366215  [0.08863, 0.21727, 0.66102, 0.82633, 0.09238]   \n",
       "\n",
       "     precision_total                                         recall  \\\n",
       "76          2.167984   [0.31148, 0.38209, 0.30534, 0.3976, 0.75159]   \n",
       "4           2.194923  [0.47541, 0.26269, 0.30009, 0.40184, 0.75796]   \n",
       "36          2.313641   [0.59016, 0.20299, 0.28959, 0.39548, 0.7707]   \n",
       "68          2.700744  [0.60656, 0.16716, 0.28871, 0.39831, 0.69427]   \n",
       "72          2.356999  [0.63934, 0.14328, 0.31584, 0.40749, 0.67516]   \n",
       "160         2.551971  [0.60656, 0.12836, 0.31146, 0.40254, 0.70701]   \n",
       "100         2.033813  [0.27869, 0.32239, 0.25809, 0.43291, 0.64968]   \n",
       "64          2.408849   [0.53279, 0.15224, 0.28346, 0.3976, 0.78344]   \n",
       "32          2.614008  [0.57377, 0.14925, 0.27822, 0.39831, 0.70701]   \n",
       "65          1.885631  [0.37705, 0.23284, 0.30709, 0.41667, 0.61783]   \n",
       "\n",
       "     recall_total                                       f1_score  \\\n",
       "76       2.148093  [0.18765, 0.28009, 0.43927, 0.55825, 0.16608]   \n",
       "4        2.197982    [0.19237, 0.26269, 0.43253, 0.5642, 0.1619]   \n",
       "36       2.248919  [0.17647, 0.25806, 0.42545, 0.56028, 0.16713]   \n",
       "68       2.155008   [0.14258, 0.27317, 0.42829, 0.56175, 0.1616]   \n",
       "72       2.181108   [0.16116, 0.22222, 0.44595, 0.5643, 0.16537]   \n",
       "160      2.155925    [0.16336, 0.215, 0.44196, 0.56492, 0.15734]   \n",
       "100      1.941760  [0.11765, 0.31304, 0.38287, 0.56917, 0.14751]   \n",
       "64       2.149529  [0.16149, 0.22667, 0.42353, 0.55964, 0.15881]   \n",
       "32       2.106551  [0.16298, 0.24752, 0.41059, 0.55759, 0.14692]   \n",
       "65       1.951473  [0.14353, 0.22478, 0.41935, 0.55399, 0.16073]   \n",
       "\n",
       "     f1_score_total                      support  \n",
       "76         1.631347  [122, 335, 1143, 1416, 157]  \n",
       "4          1.613702  [122, 335, 1143, 1416, 157]  \n",
       "36         1.587392  [122, 335, 1143, 1416, 157]  \n",
       "68         1.567400  [122, 335, 1143, 1416, 157]  \n",
       "72         1.559003  [122, 335, 1143, 1416, 157]  \n",
       "160        1.542568  [122, 335, 1143, 1416, 157]  \n",
       "100        1.530238  [122, 335, 1143, 1416, 157]  \n",
       "64         1.530141  [122, 335, 1143, 1416, 157]  \n",
       "32         1.525603  [122, 335, 1143, 1416, 157]  \n",
       "65         1.502384  [122, 335, 1143, 1416, 157]  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_third_ft.sort_values(by=['f1_score_total'], ascending=False).iloc[:10, -9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nr_train                                                      5803\n",
       "num_epochs                                                     500\n",
       "n_bad_epochs                                                     3\n",
       "num_hidden_layers                                               10\n",
       "hidden_size                                                     64\n",
       "batch_size                                                     512\n",
       "af_first_layer                      LeakyReLU(negative_slope=0.01)\n",
       "af_hidden_layers                    LeakyReLU(negative_slope=0.01)\n",
       "af_output_layer                                  LogSoftmax(dim=1)\n",
       "loss_function                                          KLDivLoss()\n",
       "dropout                                                        0.0\n",
       "batch_norm                                                   False\n",
       "learning_rate                                              0.00001\n",
       "optimizer                                         torch.optim.Adam\n",
       "weight_decay                                                0.0001\n",
       "epoch_stopped                                                   66\n",
       "loss                                                      1.316624\n",
       "accuracy                                                  0.448471\n",
       "precision            [0.11633, 0.20382, 0.48902, 0.93333, 0.15126]\n",
       "precision_total                                           1.893765\n",
       "recall               [0.46721, 0.38209, 0.56518, 0.40537, 0.11465]\n",
       "recall_total                                              1.934499\n",
       "f1_score             [0.18627, 0.26584, 0.52435, 0.56524, 0.13043]\n",
       "f1_score_total                                            1.672135\n",
       "support                                [122, 335, 1143, 1416, 157]\n",
       "Name: 41, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_second_ft.sort_values(by=['f1_score_total'], ascending=False).iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_third_ft.f1_score = [[round(z, 5) for z in list(map(float,j[1:-1].split(\", \")))] for j in results_third_ft.f1_score] \n",
    "results_third_ft.recall = [[round(z, 5) for z in list(map(float,j[1:-1].split(\", \")))] for j in results_third_ft.recall] \n",
    "results_third_ft.precision = [[round(z, 5) for z in list(map(float,j[1:-1].split(\", \")))] for j in results_third_ft.precision] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining fourth hyperparameters space for regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_hyperparams = {\n",
    "\t'num_epochs' : [500],\n",
    "\t'n_bad_epochs': [3],\n",
    "\t'num_hidden_layers' : [3, 5, 7, 10],\n",
    "\t'hidden_size' : [64, 128, 256],\n",
    "\t'batch_size' : [16, 64, 256, 512],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None],\n",
    "\t'loss_function' : [nn.MSELoss()], \n",
    "\t'dropout' : [0, 0.5],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rate' : [0.01, 1e-5], \n",
    "\t'optimizer': [\"torch.optim.Adam\"],\n",
    "\t'weight_decay': [1e-4]\t\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20956/1532733752.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mset_reproducibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mconfig_set\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mfirst_configs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mnr_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config_set' is not defined"
     ]
    }
   ],
   "source": [
    "set_reproducibility()\t\n",
    "\n",
    "if config_set == first_configs:\n",
    "\tnr_train = 0\n",
    "else :\n",
    "\tnr_train = len(configs_set0) * idx_set\n",
    "\t\n",
    "columns = [\"nr_train\"] + list(first_configs[0].keys()) + [\"epoch_stopped\", \"loss\", \"accuracy\", \"precision\", \"precision_total\", \"recall\", \"recall_total\", \"f1_score\", \"f1_score_total\", \"support\"]\n",
    "results_regression_ft = pd.DataFrame(columns=columns)\n",
    "\n",
    "for config_params in config_set:\n",
    "\tnr_train += 1\n",
    "\tprint(f\"{nr_train}° training with params:\")\n",
    "\tpprint(config_params)\n",
    "\n",
    "\tlist_params_config = list(map(str, list(config_params.values())))\n",
    "\tname_run = '__'.join(list_params_config)\n",
    "\twith SummaryWriter(log_dir=os.path.join('tensorboard_logs', f\"{idx_set}_out_of_{nr_sets - 1}\", 'Train_' + str(nr_train), name_run)) as tb:\n",
    "\t# tb = SummaryWriter(log_dir=os.path.join('tensorboard_logs', f\"{idx_set}_out_of_{nr_sets - 1}\", 'Train_' + str(nr_train), name_run))\n",
    "\n",
    "\t\ttrain_subset = Subset(dataset, train_idx)\n",
    "\t\tval_subset=Subset(dataset, val_idx)\n",
    "\t\ttest_subset=Subset(dataset, test_idx)\n",
    "\t\ttrain_loader=DataLoader(train_subset, batch_size=config_params['batch_size'], shuffle=False, sampler=sampler_class_frequency, drop_last=True)\n",
    "\t\tval_loader=DataLoader(val_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\t\ttest_loader=DataLoader(test_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "\t\tmodel = Feedforward(\n",
    "\t\t\tdataset.X.shape[1],\n",
    "\t\t\tconfig_params['hidden_size'],\n",
    "\t\t\tdataset.num_classes,\n",
    "\t\t\tconfig_params['af_first_layer'],\n",
    "\t\t\tconfig_params['af_hidden_layers'],\n",
    "\t\t\tconfig_params['af_output_layer'],\n",
    "\t\t\tconfig_params['num_hidden_layers'],\n",
    "\t\t\tconfig_params['dropout'], \n",
    "\t\t\tconfig_params['batch_norm'])\n",
    "\n",
    "\t\tmodel.to(device)\n",
    "\t\tinput_model = dataset.X[train_idx][:config_params['batch_size']].to(device)\n",
    "\t\ttb.add_graph(model, input_model)\n",
    "\n",
    "\t\tsummary(model, input_size=(config_params['batch_size'], int(35850 // config_params['batch_size']), 1149), col_names= [\"input_size\",\"output_size\", \"num_params\"], verbose=1)\n",
    "\t\t# dataset.X[train_idx].shape[1] == 1149, dataset.X[train_idx].shape[0] == 35850\t\t\tprovare verbose = 2 per weight e bias\n",
    "\t\t# test_model(model, val_loader, device)\n",
    "\n",
    "\t\tloss_func = config_params['loss_function'] \n",
    "\n",
    "\t\toptim = eval(config_params['optimizer'] + \"(model.parameters(), lr=config_params['learning_rate'])\")\n",
    "\n",
    "\t\tcardinality_training_set = len(X_train)\n",
    "\t\tmodel, loss_values, epoch_stopped, loss_value_last_epoch, accuracy_last_epoch = train_model(model, loss_func, optim, train_loader, config_params['num_epochs'], config_params['n_bad_epochs'], device, tb, cardinality_training_set)\n",
    "\t\t\n",
    "\t\tprint(f\"Loss: {loss_value_last_epoch}\", end=\"\\n\\n\")\n",
    "\n",
    "\t\treport = test_model(model, val_loader, device, True)\n",
    "\t\tindex_classes = len(report) - 3\n",
    "\n",
    "\t\tf1_score = [float(report[str(i)]['f1-score']) for i in range(index_classes)]\n",
    "\t\tf1_score_total = np.sum(f1_score)\n",
    "\n",
    "\t\tdef MSE(metrics_per_class):\n",
    "\t\t\tmean = np.mean(metrics_per_class)\n",
    "\t\t\tsum_errors_squared = 0\n",
    "\t\t\tfor j in metrics_per_class:\n",
    "\t\t\t\tsum_errors_squared += np.square(j - f1_score_mean)\n",
    "\t\t\treturn np.sqrt(sum_errors_squared)\n",
    "\n",
    "\t\tf1_score_mse = MSE(f1_score)\n",
    "\n",
    "\t\tprecision = [float(report[str(i)]['precision']) for i in range(index_classes)]\n",
    "\t\tprecision_total = np.sum(precision)\n",
    "\t\tprecision_mse = MSE(precision)\n",
    "\n",
    "\t\trecall = [float(report[str(i)]['recall']) for i in range(index_classes)]\n",
    "\t\trecall_total = np.sum(recall)\n",
    "\t\trecall_mse = MSE(recall)\n",
    "\n",
    "\t\tsupport = [int(report[str(i)]['support']) for i in range(index_classes)]\n",
    "\t\taccuracy = report['accuracy']\n",
    "\n",
    "\n",
    "\t\trow_values= [nr_train] + list_params_config + [epoch_stopped, loss_value_last_epoch, accuracy, precision, precision_total, recall, recall_total, f1_score, f1_score_total, support]\n",
    "\t\tresults_regression_ft=results_regression_ft.append(pd.Series(row_values, index=columns), ignore_index=True)\n",
    "\t\t# plt.plot(loss_values)\n",
    "\t\t# plt.title(\"Number of epochs: {}\".format(num_epochs))\n",
    "\t\t# plt.show()\n",
    "\n",
    "\t\tdict_params_config = {list(config_params.keys())[z]: list_params_config[z] for z in range(len(config_params))}\n",
    "\t\ttb.add_hparams(hparam_dict = dict_params_config, metric_dict = {\"Accuracy every epoch\": None, \"Loss every epoch\": None})\n",
    "\t\ttb.flush()\n",
    "\t\ttb.close()\n",
    "\tdel model, optim, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regression_ft.to_csv(\"tuning_hyperparams/results_regression_ft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regression_ft = pd.read_csv(\"tuning_hyperparams/results_regression_ft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results['mse_precision'] = [mean_squared_error([1:-1]) for i in results.f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = []\n",
    "for i in new_results.precision:\n",
    "\ti = list(map(float, i[1:-1].split(\", \")))\n",
    "\tsum_errors_squared = 0\n",
    "\tmean = np.mean(i)\n",
    "\tfor j in i:\n",
    "\t\tsum_errors_squared += np.square(j - mean)\n",
    "\tmse_list.append(np.sqrt(sum_errors_squared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, model):\n",
    "    row = torch.Tensor([row])\n",
    "    yhat = model(row)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
