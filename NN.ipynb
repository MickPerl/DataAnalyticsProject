{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: da fare ancora\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        df = pd.read_csv(\"script_slurm/df.csv\")\n",
    "        df = self.cleaning(df)\n",
    "\n",
    "        X, y, weights = self.split_XYweights(df)\n",
    "        \n",
    "        y = self.discretization(y)\n",
    "        X.drop('rating_mean', inplace=True, axis=1)\n",
    "\n",
    "        self.num_classes = y.nunique()\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.weights = torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx], self.weights[idx]\n",
    "\n",
    "    def split_XYweights(self, df):\n",
    "        y = df['rating_mean']\n",
    "        weights = df['ratings_count']\n",
    "        X = df.drop(columns=['ratings_count'], axis=1)\n",
    "        return X, y, weights\n",
    "\n",
    "    def cleaning(self, df):\n",
    "        df.dropna(subset=['rating_mean'], inplace=True)\n",
    "        df_without_tags = df[df.iloc[:, 23:-2].isna().all(axis = 1)]\n",
    "        df_without_tags_nor_genres = df_without_tags[\n",
    "            df_without_tags['(no genres listed)'] == 1]\n",
    "        rows_to_be_deleted = df.loc[df[\"movieId\"].isin(\n",
    "            df_without_tags_nor_genres[\"movieId\"])].index\n",
    "        df.drop(rows_to_be_deleted, axis=0, inplace=True)\n",
    "        df.iloc[:, 23:-2] = df.iloc[:, 23:-2].fillna(0)\n",
    "        df.drop(['(no genres listed)'], inplace=True, axis=1)\n",
    "        df_year_without_na = df.year[-pd.isna(df.year)]\n",
    "        df.year = df.loc[:, 'year'].fillna(\n",
    "            np.median(df_year_without_na)).astype('int')\n",
    "        df.drop('movieId', inplace=True, axis=1)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def discretization(self, series):\n",
    "        series = pd.cut(series, bins=5, labels=False)\n",
    "        return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout):\n",
    "        super(Feedforward, self).__init__()\n",
    "    \n",
    "        model = [nn.Linear(input_size, hidden_size), af_first_layer]\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            model.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "            if batch_norm:\n",
    "                model.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            model.append(af_hidden_layers)\n",
    "            \n",
    "            if dropout != 0:\n",
    "                model.append(nn.Dropout(dropout))\n",
    "    \n",
    "\n",
    "        model.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if af_output_layer :\n",
    "            model.append(af_output_layer)\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs, data_loader, device):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    n_epochs_stop = 3\n",
    "    patience = 0\n",
    "    early_stop = False\n",
    "    min_loss = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        for batch_idx, samples in enumerate(data_loader):\n",
    "            data, targets = samples[0].to(device), samples[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "            # y_predd = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_current_batch = loss.item()\n",
    "\n",
    "            # If the validation loss is at a minimum\n",
    "            if loss_current_batch < min_loss:\n",
    "                # Save the model\n",
    "                # torch.save(model)\n",
    "                if min_loss - loss_current_batch >= 1:\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                min_loss = loss_current_batch\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "            if epoch > 1 and patience >= n_epochs_stop:\n",
    "                print(f\"Epoca: {epoch}, patience: {patience}, n stop: {n_epochs_stop}\")\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "\n",
    "        if early_stop:\n",
    "            print(f'Early stopping at epoch number {epoch}!')\n",
    "            break\n",
    "\n",
    "    return model, loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_val = []\n",
    "\n",
    "    for batch_idx, samples in enumerate(data_loader):\n",
    "        data, targets = samples[0].to(device), samples[1].to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_val.append(targets)\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    y_val = torch.stack(y_val).squeeze()\n",
    "    y_pred = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "    print(classification_report(y_val.cpu(), y_pred.cpu(), zero_division=0))\n",
    "\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for batch_idx, samples in enumerate(data_loader):\n",
    "        data, targets = samples[0].to(device), samples[1].to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_test.append(targets)\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "    print(classification_report(y_test.cpu(), y_pred.cpu(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_reproducibility(seed = 42):\n",
    "\ttorch.manual_seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\tos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\ttorch.use_deterministic_algorithms(True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "hidden_size = 8\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "dropout = 0\n",
    "batch_norm = False\n",
    "af_first_layer = nn.LeakyReLU()\n",
    "af_hidden_layers = nn.LeakyReLU()\n",
    "af_output_layer = None\n",
    "num_hidden_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "\t'nums_hidden_layers' : [1, 3, 5, 7, 10],\n",
    "\t'hidden_sizes' : [8, 16, 32, 64, 128],\n",
    "\t'batch_sizes' : [32, 64, 128, 256, 512],\n",
    "\t'af_first_layer' : [nn.Tanh(), nn.LeakyReLU()],\n",
    "\t'af_hidden_layers' : [nn.LeakyReLU()],\n",
    "\t'af_output_layer' : [None], # [None, nn.Softmax(dim=1)],\n",
    "\t'loss_function' : [nn.CrossEntropyLoss()], #[nn.CrossEntropyLoss(), nn.KLDivLoss(reduction = 'batchmean')],\n",
    "\t'dropout' : [0, 0.2, 0.4],\n",
    "\t'batch_norm' : [False, True],\n",
    "\t'learning_rates' : [0.01]\n",
    "}\n",
    "\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "dataset = MoviesDataset()\n",
    "train_idx, test_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.y, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=dataset.y[train_idx], random_state=42)\n",
    "\n",
    "# MinMaxScale training, validation and testing set su year e title_length\n",
    "X_train = dataset.X[train_idx]\n",
    "X_val = dataset.X[val_idx]\n",
    "X_test = dataset.X[test_idx]\n",
    "\n",
    "train_year_max = torch.max(X_train[:,1])\n",
    "train_year_min = torch.min(X_train[:,1])\n",
    "dataset.X[train_idx, 1] = (X_train[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[val_idx, 1] = (X_val[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "dataset.X[test_idx, 1] = (X_test[:,1] - train_year_min)/(train_year_max - train_year_min)\n",
    "\n",
    "train_title_length_max = torch.max(X_train[:,2])\n",
    "train_title_length_min = torch.min(X_train[:,2])\n",
    "dataset.X[train_idx, 2] = (X_train[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[val_idx, 2] = (X_val[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)\n",
    "dataset.X[test_idx, 2] = (X_test[:,2] - train_title_length_min)/(train_title_length_max - train_title_length_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating samplers to manage unbalancing classes\n",
    "def class_weights(y):\n",
    "    class_count = torch.bincount(y)\n",
    "    class_weighting = 1. / class_count\n",
    "    sample_weights = class_weighting[y]   # sarebbe np.array([weight[t] for t in y_train])\n",
    "    return sample_weights\n",
    "\n",
    "y_train = dataset.y[train_idx]\n",
    "\n",
    "sample_weights = class_weights(y_train)\n",
    "sampler_class_frequency = WeightedRandomSampler(sample_weights, len(train_idx))\n",
    "\n",
    "# MinMaxScaling ratings_count\n",
    "weights_train = dataset.weights[train_idx] \n",
    "weights_val = dataset.weights[val_idx]\n",
    "weights_test = dataset.weights[test_idx] \n",
    "\n",
    "weights_train_max = torch.max(weights_train)\n",
    "weights_train_min = torch.min(weights_train)\n",
    "dataset.weights[train_idx]  = (weights_train - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[val_idx] = (weights_val - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "dataset.weights[test_idx] = (weights_test - weights_train_min) / (weights_train_max - weights_train_min)\n",
    "\n",
    "sampler_ratings_count = WeightedRandomSampler(dataset.weights[train_idx], len(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W Context.cpp:70] Warning: torch.use_deterministic_algorithms is in beta, and its design and functionality may change in the future. (function operator())\n"
     ]
    }
   ],
   "source": [
    "set_reproducibility()\n",
    "\n",
    "train_subset = Subset(dataset, train_idx)\n",
    "val_subset = Subset(dataset, val_idx)\n",
    "test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, sampler=sampler_class_frequency, drop_last=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=1, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\t7\t5\t4\t9\n",
      "5\t3\t9\t10\t5\n",
      "4\t3\t11\t6\t8\n",
      "6\t7\t7\t4\t8\n",
      "2\t10\t4\t8\t8\n",
      "7\t4\t6\t10\t5\n",
      "4\t3\t8\t8\t9\n",
      "7\t6\t6\t8\t5\n",
      "8\t5\t8\t6\t5\n",
      "9\t7\t5\t6\t5\n",
      "4\t8\t6\t7\t7\n",
      "6\t5\t10\t7\t4\n",
      "8\t6\t5\t8\t5\n",
      "10\t6\t6\t4\t6\n",
      "4\t5\t6\t9\t8\n",
      "9\t4\t6\t5\t8\n",
      "7\t7\t6\t9\t3\n",
      "7\t5\t5\t10\t5\n",
      "5\t9\t6\t8\t4\n",
      "6\t3\t2\t9\t12\n",
      "5\t5\t5\t5\t12\n",
      "7\t7\t6\t4\t8\n",
      "4\t6\t7\t8\t7\n",
      "6\t7\t6\t8\t5\n",
      "9\t5\t5\t6\t7\n",
      "5\t7\t7\t8\t5\n",
      "5\t6\t9\t5\t7\n",
      "4\t6\t8\t6\t8\n",
      "2\t11\t4\t9\t6\n",
      "3\t5\t9\t9\t6\n",
      "5\t7\t9\t5\t6\n",
      "5\t11\t4\t8\t4\n",
      "7\t7\t7\t6\t5\n",
      "7\t8\t6\t4\t7\n",
      "6\t3\t11\t5\t7\n",
      "6\t5\t7\t8\t6\n",
      "6\t7\t7\t7\t5\n",
      "9\t7\t7\t4\t5\n",
      "6\t7\t7\t5\t7\n",
      "4\t6\t10\t3\t9\n",
      "4\t8\t6\t10\t4\n",
      "5\t9\t7\t8\t3\n",
      "4\t6\t8\t8\t6\n",
      "4\t6\t12\t7\t3\n",
      "4\t9\t5\t6\t8\n",
      "6\t5\t7\t8\t6\n",
      "10\t6\t6\t7\t3\n",
      "7\t11\t7\t2\t5\n",
      "6\t5\t5\t5\t11\n",
      "5\t5\t8\t8\t6\n",
      "10\t5\t7\t4\t6\n",
      "8\t7\t3\t3\t11\n",
      "4\t5\t7\t10\t6\n",
      "6\t6\t5\t9\t6\n",
      "4\t4\t10\t7\t7\n",
      "6\t6\t6\t9\t5\n",
      "4\t9\t5\t7\t7\n",
      "7\t10\t3\t4\t8\n",
      "10\t8\t8\t4\t2\n",
      "3\t7\t8\t4\t10\n",
      "5\t1\t7\t8\t11\n",
      "5\t8\t7\t8\t4\n",
      "5\t7\t6\t7\t7\n",
      "11\t10\t6\t3\t2\n",
      "4\t5\t6\t13\t4\n",
      "9\t4\t8\t7\t4\n",
      "6\t10\t4\t6\t6\n",
      "7\t5\t8\t4\t8\n",
      "5\t6\t5\t8\t8\n",
      "5\t7\t4\t9\t7\n",
      "3\t9\t6\t6\t8\n",
      "6\t3\t8\t4\t11\n",
      "5\t6\t6\t4\t11\n",
      "8\t3\t6\t5\t10\n",
      "8\t5\t8\t5\t6\n",
      "6\t4\t10\t4\t8\n",
      "7\t6\t9\t3\t7\n",
      "8\t5\t3\t6\t10\n",
      "10\t7\t5\t4\t6\n",
      "8\t9\t6\t3\t6\n",
      "5\t10\t6\t5\t6\n",
      "2\t7\t10\t9\t4\n",
      "6\t10\t5\t7\t4\n",
      "7\t9\t5\t5\t6\n",
      "10\t4\t3\t7\t8\n",
      "3\t9\t9\t8\t3\n",
      "4\t8\t9\t6\t5\n",
      "4\t8\t8\t4\t8\n",
      "7\t6\t8\t4\t7\n",
      "8\t7\t7\t3\t7\n",
      "5\t8\t5\t7\t7\n",
      "8\t10\t3\t4\t7\n",
      "5\t9\t4\t5\t9\n",
      "4\t11\t6\t7\t4\n",
      "6\t8\t7\t6\t5\n",
      "3\t8\t8\t6\t7\n",
      "11\t8\t7\t5\t1\n",
      "6\t9\t4\t7\t6\n",
      "7\t6\t6\t9\t4\n",
      "4\t7\t4\t11\t6\n",
      "6\t6\t4\t6\t10\n",
      "8\t3\t9\t6\t6\n",
      "5\t7\t5\t7\t8\n",
      "6\t7\t3\t9\t7\n",
      "6\t2\t7\t4\t13\n",
      "4\t6\t4\t11\t7\n",
      "6\t7\t3\t6\t10\n",
      "6\t7\t4\t5\t10\n",
      "10\t4\t8\t6\t4\n",
      "3\t8\t5\t7\t9\n",
      "3\t8\t10\t5\t6\n",
      "7\t7\t7\t4\t7\n",
      "9\t5\t2\t7\t9\n",
      "4\t8\t10\t5\t5\n",
      "5\t5\t7\t4\t11\n",
      "6\t5\t6\t7\t8\n",
      "4\t5\t7\t10\t6\n",
      "4\t11\t6\t5\t6\n",
      "7\t6\t3\t9\t7\n",
      "6\t6\t6\t7\t7\n",
      "4\t4\t10\t6\t8\n",
      "5\t10\t5\t5\t7\n",
      "4\t4\t7\t8\t9\n",
      "7\t9\t6\t6\t4\n",
      "6\t6\t8\t9\t3\n",
      "6\t7\t5\t8\t6\n",
      "5\t6\t6\t10\t5\n",
      "8\t7\t9\t2\t6\n",
      "9\t5\t7\t8\t3\n",
      "6\t5\t9\t6\t6\n",
      "7\t8\t5\t8\t4\n",
      "4\t5\t9\t8\t6\n",
      "8\t7\t4\t6\t7\n",
      "8\t8\t9\t2\t5\n",
      "2\t10\t9\t8\t3\n",
      "7\t6\t7\t8\t4\n",
      "8\t6\t5\t7\t6\n",
      "5\t6\t7\t8\t6\n",
      "4\t1\t12\t8\t7\n",
      "9\t5\t9\t4\t5\n",
      "6\t8\t10\t4\t4\n",
      "4\t5\t5\t11\t7\n",
      "5\t4\t7\t9\t7\n",
      "10\t10\t5\t3\t4\n",
      "3\t6\t8\t10\t5\n",
      "5\t9\t6\t6\t6\n",
      "8\t7\t7\t4\t6\n",
      "7\t5\t9\t3\t8\n",
      "5\t8\t7\t7\t5\n",
      "8\t6\t7\t5\t6\n",
      "2\t7\t8\t9\t6\n",
      "8\t4\t5\t4\t11\n",
      "7\t7\t8\t7\t3\n",
      "4\t6\t9\t5\t8\n",
      "8\t4\t7\t6\t7\n",
      "6\t9\t6\t5\t6\n",
      "4\t8\t6\t8\t6\n",
      "7\t9\t3\t5\t8\n",
      "4\t6\t6\t9\t7\n",
      "7\t11\t2\t9\t3\n",
      "8\t10\t9\t4\t1\n",
      "10\t7\t8\t5\t2\n",
      "8\t4\t4\t9\t7\n",
      "5\t10\t3\t6\t8\n",
      "6\t6\t8\t6\t6\n",
      "8\t5\t6\t7\t6\n",
      "5\t13\t5\t7\t2\n",
      "5\t3\t5\t7\t12\n",
      "8\t6\t4\t5\t9\n",
      "3\t10\t5\t8\t6\n",
      "8\t6\t4\t8\t6\n",
      "11\t5\t6\t3\t7\n",
      "4\t9\t3\t11\t5\n",
      "8\t8\t6\t7\t3\n",
      "8\t7\t10\t1\t6\n",
      "6\t6\t6\t9\t5\n",
      "6\t7\t5\t4\t10\n",
      "9\t1\t6\t10\t6\n",
      "4\t4\t5\t9\t10\n",
      "8\t7\t3\t6\t8\n",
      "7\t6\t9\t4\t6\n",
      "7\t5\t7\t4\t9\n",
      "12\t4\t9\t4\t3\n",
      "9\t6\t8\t7\t2\n",
      "8\t4\t6\t9\t5\n",
      "4\t8\t6\t5\t9\n",
      "12\t3\t4\t8\t5\n",
      "8\t7\t4\t7\t6\n",
      "10\t10\t4\t4\t4\n",
      "6\t6\t7\t7\t6\n",
      "7\t7\t8\t7\t3\n",
      "4\t10\t5\t9\t4\n",
      "4\t5\t11\t6\t6\n",
      "7\t7\t7\t3\t8\n",
      "8\t8\t4\t6\t6\n",
      "7\t2\t5\t7\t11\n",
      "4\t6\t7\t11\t4\n",
      "5\t11\t6\t6\t4\n",
      "6\t7\t8\t7\t4\n",
      "2\t8\t9\t7\t6\n",
      "7\t9\t6\t5\t5\n",
      "11\t4\t6\t3\t8\n",
      "8\t6\t6\t9\t3\n",
      "4\t7\t6\t13\t2\n",
      "4\t11\t6\t5\t6\n",
      "4\t9\t7\t5\t7\n",
      "7\t5\t5\t6\t9\n",
      "9\t3\t6\t7\t7\n",
      "4\t6\t8\t9\t5\n",
      "4\t10\t6\t5\t7\n",
      "6\t6\t7\t6\t7\n",
      "5\t6\t8\t4\t9\n",
      "5\t8\t4\t7\t8\n",
      "2\t9\t4\t11\t6\n",
      "7\t3\t12\t4\t6\n",
      "8\t8\t5\t3\t8\n",
      "9\t4\t5\t7\t7\n",
      "6\t5\t5\t6\t10\n",
      "6\t3\t8\t13\t2\n",
      "4\t7\t5\t7\t9\n",
      "9\t10\t3\t3\t7\n",
      "8\t7\t2\t6\t9\n",
      "8\t6\t4\t8\t6\n",
      "9\t10\t3\t3\t7\n",
      "5\t8\t7\t7\t5\n",
      "6\t4\t7\t8\t7\n",
      "7\t5\t5\t10\t5\n",
      "2\t7\t6\t8\t9\n",
      "6\t2\t9\t8\t7\n",
      "3\t13\t7\t3\t6\n",
      "6\t6\t8\t7\t5\n",
      "4\t4\t8\t7\t9\n",
      "5\t10\t6\t4\t7\n",
      "4\t6\t2\t9\t11\n",
      "8\t4\t7\t6\t7\n",
      "4\t7\t3\t9\t9\n",
      "8\t4\t11\t1\t8\n",
      "7\t8\t6\t6\t5\n",
      "4\t4\t4\t12\t8\n",
      "7\t14\t3\t5\t3\n",
      "11\t4\t5\t7\t5\n",
      "8\t6\t6\t4\t8\n",
      "4\t8\t7\t8\t5\n",
      "9\t6\t5\t4\t8\n",
      "7\t2\t9\t10\t4\n",
      "7\t7\t3\t9\t6\n",
      "8\t5\t3\t9\t7\n",
      "3\t7\t7\t8\t7\n",
      "1\t9\t4\t6\t12\n",
      "10\t4\t5\t5\t8\n",
      "4\t5\t7\t7\t9\n",
      "6\t9\t5\t5\t7\n",
      "7\t3\t9\t6\t7\n",
      "8\t7\t7\t6\t4\n",
      "5\t8\t8\t4\t7\n",
      "3\t5\t9\t9\t6\n",
      "7\t7\t3\t11\t4\n",
      "5\t6\t8\t7\t6\n",
      "4\t7\t7\t6\t8\n",
      "7\t7\t6\t6\t6\n",
      "6\t7\t6\t3\t10\n",
      "7\t6\t4\t4\t11\n",
      "8\t4\t10\t6\t4\n",
      "8\t6\t5\t9\t4\n",
      "11\t4\t5\t4\t8\n",
      "7\t8\t4\t7\t6\n",
      "3\t4\t13\t5\t7\n",
      "6\t3\t7\t7\t9\n",
      "7\t11\t5\t5\t4\n",
      "3\t7\t5\t10\t7\n",
      "6\t7\t6\t1\t12\n",
      "8\t7\t10\t5\t2\n",
      "8\t5\t8\t7\t4\n",
      "7\t5\t6\t7\t7\n",
      "8\t6\t2\t8\t8\n",
      "6\t6\t7\t4\t9\n",
      "8\t6\t6\t4\t8\n",
      "6\t4\t9\t7\t6\n",
      "4\t5\t8\t7\t8\n",
      "10\t5\t10\t3\t4\n",
      "1\t10\t6\t9\t6\n",
      "8\t4\t5\t6\t9\n",
      "4\t8\t7\t6\t7\n",
      "5\t5\t9\t5\t8\n",
      "8\t4\t4\t8\t8\n",
      "6\t4\t6\t10\t6\n",
      "7\t6\t8\t7\t4\n",
      "2\t7\t5\t5\t13\n",
      "5\t5\t10\t5\t7\n",
      "3\t5\t10\t9\t5\n",
      "11\t5\t9\t2\t5\n",
      "5\t6\t8\t6\t7\n",
      "5\t5\t6\t10\t6\n",
      "8\t6\t5\t4\t9\n",
      "10\t6\t3\t9\t4\n",
      "5\t6\t7\t6\t8\n",
      "6\t8\t5\t5\t8\n",
      "5\t4\t6\t13\t4\n",
      "4\t2\t5\t10\t11\n",
      "8\t3\t10\t8\t3\n",
      "7\t5\t7\t8\t5\n",
      "11\t3\t7\t6\t5\n",
      "8\t8\t6\t5\t5\n",
      "6\t7\t5\t7\t7\n",
      "7\t3\t10\t3\t9\n",
      "3\t8\t5\t9\t7\n",
      "7\t5\t10\t6\t4\n",
      "9\t4\t5\t5\t9\n",
      "9\t8\t1\t9\t5\n",
      "3\t6\t7\t9\t7\n",
      "8\t4\t8\t8\t4\n",
      "9\t7\t6\t5\t5\n",
      "9\t7\t6\t6\t4\n",
      "4\t12\t4\t7\t5\n",
      "9\t5\t7\t7\t4\n",
      "7\t4\t6\t11\t4\n",
      "6\t7\t5\t6\t8\n",
      "7\t11\t9\t3\t2\n",
      "5\t5\t5\t7\t10\n",
      "6\t5\t7\t8\t6\n",
      "4\t6\t6\t8\t8\n",
      "7\t6\t2\t8\t9\n",
      "6\t6\t8\t7\t5\n",
      "14\t5\t5\t6\t2\n",
      "6\t6\t8\t4\t8\n",
      "8\t3\t10\t5\t6\n",
      "6\t3\t6\t5\t12\n",
      "8\t8\t3\t7\t6\n",
      "7\t4\t3\t8\t10\n",
      "4\t10\t5\t8\t5\n",
      "8\t6\t7\t8\t3\n",
      "7\t6\t9\t4\t6\n",
      "6\t6\t9\t7\t4\n",
      "4\t8\t5\t6\t9\n",
      "10\t4\t7\t5\t6\n",
      "4\t10\t3\t9\t6\n",
      "5\t6\t6\t7\t8\n",
      "12\t6\t6\t4\t4\n",
      "6\t9\t4\t5\t8\n",
      "2\t8\t7\t7\t8\n",
      "4\t7\t7\t7\t7\n",
      "6\t6\t8\t6\t6\n",
      "8\t10\t2\t5\t7\n",
      "10\t9\t3\t4\t6\n",
      "10\t7\t2\t6\t7\n",
      "4\t2\t7\t9\t10\n",
      "8\t9\t4\t4\t7\n",
      "8\t8\t6\t5\t5\n",
      "6\t10\t7\t3\t6\n",
      "4\t4\t7\t3\t14\n",
      "5\t12\t5\t6\t4\n",
      "7\t4\t10\t7\t4\n",
      "6\t6\t7\t6\t7\n",
      "2\t7\t7\t9\t7\n",
      "1\t13\t11\t4\t3\n",
      "7\t5\t5\t10\t5\n",
      "3\t12\t7\t5\t5\n",
      "4\t10\t5\t4\t9\n",
      "4\t3\t11\t6\t8\n",
      "6\t6\t9\t6\t5\n",
      "6\t6\t5\t9\t6\n",
      "8\t4\t5\t7\t8\n",
      "6\t10\t3\t5\t8\n",
      "9\t5\t7\t6\t5\n",
      "9\t7\t9\t5\t2\n",
      "5\t3\t13\t4\t7\n",
      "5\t11\t5\t6\t5\n",
      "8\t8\t7\t3\t6\n",
      "8\t4\t8\t7\t5\n",
      "4\t7\t5\t4\t12\n",
      "9\t5\t4\t8\t6\n",
      "5\t4\t8\t9\t6\n",
      "11\t3\t5\t8\t5\n",
      "8\t5\t9\t3\t7\n",
      "8\t4\t5\t2\t13\n",
      "7\t4\t6\t6\t9\n",
      "10\t8\t5\t3\t6\n",
      "6\t7\t5\t7\t7\n",
      "9\t3\t9\t7\t4\n",
      "6\t5\t9\t5\t7\n",
      "5\t5\t8\t7\t7\n",
      "8\t8\t7\t3\t6\n",
      "7\t7\t6\t6\t6\n",
      "7\t9\t7\t7\t2\n",
      "5\t7\t5\t7\t8\n",
      "7\t8\t9\t5\t3\n",
      "8\t7\t1\t10\t6\n",
      "9\t7\t4\t6\t6\n",
      "10\t2\t3\t8\t9\n",
      "7\t5\t7\t8\t5\n",
      "6\t8\t5\t4\t9\n",
      "6\t7\t7\t5\t7\n",
      "5\t8\t7\t4\t8\n",
      "7\t4\t4\t8\t9\n",
      "3\t5\t8\t9\t7\n",
      "2\t6\t6\t7\t11\n",
      "6\t8\t4\t8\t6\n",
      "7\t8\t3\t4\t10\n",
      "6\t9\t8\t6\t3\n",
      "4\t13\t2\t8\t5\n",
      "9\t8\t1\t7\t7\n",
      "9\t4\t6\t4\t9\n",
      "7\t9\t6\t6\t4\n",
      "6\t7\t9\t6\t4\n",
      "6\t5\t8\t6\t7\n",
      "6\t8\t10\t1\t7\n",
      "5\t4\t8\t5\t10\n",
      "5\t10\t5\t5\t7\n",
      "3\t6\t6\t9\t8\n",
      "7\t5\t6\t7\t7\n",
      "12\t7\t3\t2\t8\n",
      "4\t7\t8\t7\t6\n",
      "9\t7\t8\t2\t6\n",
      "6\t6\t8\t7\t5\n",
      "7\t5\t5\t8\t7\n",
      "11\t9\t5\t2\t5\n",
      "6\t9\t7\t4\t6\n",
      "8\t8\t4\t4\t8\n",
      "5\t7\t8\t7\t5\n",
      "7\t8\t4\t6\t7\n",
      "10\t5\t6\t7\t4\n",
      "8\t7\t5\t3\t9\n",
      "8\t4\t8\t5\t7\n",
      "3\t4\t7\t10\t8\n",
      "5\t6\t6\t12\t3\n",
      "7\t5\t10\t6\t4\n",
      "13\t7\t6\t3\t3\n",
      "7\t4\t8\t6\t7\n",
      "7\t8\t7\t4\t6\n",
      "8\t7\t8\t6\t3\n",
      "7\t5\t8\t7\t5\n",
      "4\t2\t14\t7\t5\n",
      "8\t9\t5\t6\t4\n",
      "4\t8\t9\t6\t5\n",
      "12\t4\t5\t2\t9\n",
      "4\t6\t5\t9\t8\n",
      "7\t6\t10\t5\t4\n",
      "4\t11\t7\t9\t1\n",
      "5\t4\t6\t5\t12\n",
      "6\t8\t5\t6\t7\n",
      "4\t9\t5\t4\t10\n",
      "6\t9\t7\t7\t3\n",
      "8\t7\t6\t7\t4\n",
      "7\t9\t3\t8\t5\n",
      "3\t13\t5\t7\t4\n",
      "7\t4\t6\t4\t11\n",
      "6\t8\t6\t6\t6\n",
      "6\t11\t3\t5\t7\n",
      "5\t8\t7\t6\t6\n",
      "8\t7\t6\t6\t5\n",
      "6\t7\t7\t2\t10\n",
      "7\t6\t5\t8\t6\n",
      "13\t5\t2\t7\t5\n",
      "5\t6\t11\t5\t5\n",
      "7\t8\t4\t8\t5\n",
      "5\t8\t3\t6\t10\n",
      "10\t6\t8\t3\t5\n",
      "7\t8\t3\t6\t8\n",
      "5\t9\t6\t6\t6\n",
      "9\t6\t7\t6\t4\n",
      "7\t10\t3\t10\t2\n",
      "7\t4\t9\t8\t4\n",
      "8\t3\t5\t8\t8\n",
      "5\t5\t8\t7\t7\n",
      "6\t8\t6\t4\t8\n",
      "3\t6\t10\t7\t6\n",
      "6\t8\t8\t7\t3\n",
      "6\t6\t7\t10\t3\n",
      "10\t6\t5\t6\t5\n",
      "7\t8\t8\t4\t5\n",
      "8\t8\t4\t4\t8\n",
      "7\t4\t5\t8\t8\n",
      "7\t2\t6\t6\t11\n",
      "5\t9\t5\t5\t8\n",
      "11\t3\t6\t6\t6\n",
      "4\t6\t4\t8\t10\n",
      "7\t6\t4\t7\t8\n",
      "7\t7\t5\t9\t4\n",
      "6\t9\t6\t4\t7\n",
      "6\t6\t7\t7\t6\n",
      "6\t9\t8\t6\t3\n",
      "4\t3\t10\t7\t8\n",
      "7\t5\t8\t5\t7\n",
      "4\t7\t10\t4\t7\n",
      "7\t8\t5\t7\t5\n",
      "9\t8\t4\t6\t5\n",
      "7\t5\t5\t8\t7\n",
      "8\t7\t4\t6\t7\n",
      "8\t2\t8\t8\t6\n",
      "5\t5\t7\t7\t8\n",
      "11\t7\t8\t4\t2\n",
      "5\t6\t8\t5\t8\n",
      "10\t5\t6\t6\t5\n",
      "4\t6\t7\t7\t8\n",
      "8\t8\t4\t9\t3\n",
      "7\t8\t6\t4\t7\n",
      "10\t5\t3\t7\t7\n",
      "12\t4\t6\t4\t6\n",
      "5\t6\t6\t7\t8\n",
      "9\t9\t5\t4\t5\n",
      "7\t6\t7\t6\t6\n",
      "7\t6\t6\t6\t7\n",
      "5\t8\t3\t9\t7\n",
      "7\t6\t6\t6\t7\n",
      "8\t5\t5\t9\t5\n",
      "4\t13\t4\t7\t4\n",
      "4\t4\t12\t10\t2\n",
      "8\t4\t10\t3\t7\n",
      "3\t8\t8\t6\t7\n",
      "6\t5\t3\t8\t10\n",
      "4\t7\t9\t6\t6\n",
      "7\t7\t4\t7\t7\n",
      "4\t9\t3\t4\t12\n",
      "3\t9\t7\t5\t8\n",
      "9\t3\t3\t11\t6\n",
      "6\t11\t5\t6\t4\n",
      "5\t6\t5\t6\t10\n",
      "5\t11\t6\t7\t3\n",
      "4\t7\t12\t3\t6\n",
      "5\t5\t7\t7\t8\n",
      "4\t7\t5\t7\t9\n",
      "4\t11\t6\t8\t3\n",
      "6\t6\t5\t11\t4\n",
      "8\t7\t7\t3\t7\n",
      "5\t4\t8\t9\t6\n",
      "6\t7\t6\t7\t6\n",
      "7\t9\t7\t8\t1\n",
      "11\t7\t4\t8\t2\n",
      "6\t8\t8\t6\t4\n",
      "2\t11\t7\t5\t7\n",
      "8\t7\t6\t5\t6\n",
      "5\t7\t7\t7\t6\n",
      "10\t7\t2\t6\t7\n",
      "6\t4\t8\t9\t5\n",
      "5\t8\t4\t6\t9\n",
      "10\t2\t6\t6\t8\n",
      "6\t10\t2\t5\t9\n",
      "5\t6\t6\t9\t6\n",
      "7\t8\t5\t8\t4\n",
      "8\t3\t7\t7\t7\n",
      "16\t6\t4\t4\t2\n",
      "7\t7\t6\t7\t5\n",
      "6\t7\t6\t7\t6\n",
      "4\t5\t8\t6\t9\n",
      "13\t4\t3\t9\t3\n",
      "11\t2\t10\t7\t2\n",
      "7\t6\t4\t5\t10\n",
      "13\t5\t5\t4\t5\n",
      "9\t12\t3\t5\t3\n",
      "7\t8\t8\t1\t8\n",
      "5\t5\t7\t7\t8\n",
      "6\t5\t9\t5\t7\n",
      "3\t8\t7\t7\t7\n",
      "7\t6\t7\t4\t8\n",
      "5\t6\t5\t11\t5\n",
      "7\t11\t2\t7\t5\n",
      "5\t6\t8\t8\t5\n",
      "9\t4\t10\t3\t6\n",
      "10\t7\t7\t5\t3\n",
      "8\t7\t7\t4\t6\n",
      "13\t4\t2\t3\t10\n",
      "6\t7\t7\t7\t5\n",
      "4\t8\t6\t6\t8\n",
      "4\t11\t5\t6\t6\n",
      "4\t7\t6\t10\t5\n",
      "6\t4\t9\t8\t5\n",
      "6\t5\t6\t9\t6\n",
      "7\t6\t8\t7\t4\n",
      "7\t7\t5\t6\t7\n",
      "2\t1\t14\t7\t8\n",
      "8\t8\t7\t6\t3\n",
      "6\t7\t6\t6\t7\n",
      "4\t8\t10\t7\t3\n",
      "3\t11\t3\t11\t4\n",
      "4\t6\t6\t9\t7\n",
      "4\t10\t5\t8\t5\n",
      "4\t10\t5\t6\t7\n",
      "6\t6\t4\t6\t10\n",
      "9\t4\t7\t6\t6\n",
      "4\t7\t5\t6\t10\n",
      "7\t8\t8\t2\t7\n",
      "8\t7\t3\t6\t8\n",
      "8\t5\t8\t5\t6\n",
      "9\t8\t1\t8\t6\n",
      "4\t11\t5\t3\t9\n",
      "4\t6\t10\t4\t8\n",
      "7\t6\t9\t6\t4\n",
      "3\t7\t7\t8\t7\n",
      "10\t7\t6\t6\t3\n",
      "9\t5\t7\t5\t6\n",
      "7\t7\t7\t6\t5\n",
      "11\t7\t3\t8\t3\n",
      "3\t8\t4\t9\t8\n",
      "5\t6\t7\t7\t7\n",
      "7\t9\t5\t1\t10\n",
      "8\t9\t4\t4\t7\n",
      "5\t7\t5\t6\t9\n",
      "4\t10\t2\t11\t5\n",
      "2\t5\t11\t5\t9\n",
      "6\t5\t9\t6\t6\n",
      "9\t6\t2\t7\t8\n",
      "6\t5\t12\t5\t4\n",
      "7\t6\t5\t7\t7\n",
      "5\t8\t6\t5\t8\n",
      "8\t3\t7\t10\t4\n",
      "9\t3\t8\t7\t5\n",
      "8\t4\t5\t10\t5\n",
      "10\t5\t6\t5\t6\n",
      "6\t4\t10\t7\t5\n",
      "7\t9\t6\t3\t7\n",
      "6\t7\t9\t5\t5\n",
      "5\t3\t8\t8\t8\n",
      "3\t9\t7\t8\t5\n",
      "8\t9\t5\t6\t4\n",
      "9\t3\t6\t7\t7\n",
      "6\t7\t9\t8\t2\n",
      "5\t5\t11\t9\t2\n",
      "4\t8\t8\t4\t8\n",
      "6\t3\t8\t7\t8\n",
      "10\t4\t4\t9\t5\n",
      "5\t6\t11\t4\t6\n",
      "7\t9\t8\t2\t6\n",
      "12\t4\t6\t3\t7\n",
      "8\t10\t7\t2\t5\n",
      "4\t10\t6\t4\t8\n",
      "1\t8\t3\t12\t8\n",
      "12\t6\t4\t5\t5\n",
      "2\t11\t6\t4\t9\n",
      "6\t9\t6\t6\t5\n",
      "7\t8\t7\t5\t5\n",
      "6\t6\t9\t8\t3\n",
      "7\t9\t5\t5\t6\n",
      "7\t6\t11\t2\t6\n",
      "6\t6\t4\t9\t7\n",
      "8\t3\t8\t5\t8\n",
      "7\t3\t11\t8\t3\n",
      "6\t10\t4\t5\t7\n",
      "6\t10\t7\t3\t6\n",
      "8\t6\t6\t5\t7\n",
      "9\t5\t9\t3\t6\n",
      "6\t6\t6\t4\t10\n",
      "4\t6\t6\t4\t12\n",
      "4\t9\t7\t6\t6\n",
      "8\t6\t9\t4\t5\n",
      "7\t9\t1\t11\t4\n",
      "7\t7\t5\t4\t9\n",
      "3\t6\t10\t6\t7\n",
      "6\t5\t7\t7\t7\n",
      "4\t9\t6\t6\t7\n",
      "8\t6\t6\t7\t5\n",
      "6\t6\t6\t9\t5\n",
      "9\t7\t3\t8\t5\n",
      "8\t8\t4\t6\t6\n",
      "6\t7\t8\t8\t3\n",
      "5\t7\t7\t7\t6\n",
      "6\t8\t4\t6\t8\n",
      "7\t4\t5\t5\t11\n",
      "6\t6\t7\t4\t9\n",
      "7\t7\t5\t8\t5\n",
      "3\t6\t7\t8\t8\n",
      "5\t11\t3\t5\t8\n",
      "5\t8\t7\t6\t6\n",
      "3\t8\t11\t7\t3\n",
      "5\t3\t8\t5\t11\n",
      "6\t6\t9\t6\t5\n",
      "7\t8\t8\t5\t4\n",
      "4\t5\t7\t10\t6\n",
      "2\t6\t10\t8\t6\n",
      "7\t6\t4\t8\t7\n",
      "7\t5\t7\t8\t5\n",
      "5\t5\t9\t9\t4\n",
      "6\t7\t8\t9\t2\n",
      "5\t5\t13\t3\t6\n",
      "5\t9\t3\t6\t9\n",
      "5\t6\t7\t10\t4\n",
      "5\t7\t5\t5\t10\n",
      "5\t9\t9\t4\t5\n",
      "6\t6\t5\t7\t8\n",
      "6\t9\t3\t4\t10\n",
      "4\t8\t7\t8\t5\n",
      "4\t8\t8\t6\t6\n",
      "7\t6\t4\t8\t7\n",
      "6\t5\t6\t7\t8\n",
      "11\t5\t4\t6\t6\n",
      "4\t11\t7\t5\t5\n",
      "8\t5\t6\t8\t5\n",
      "5\t5\t7\t9\t6\n",
      "11\t7\t4\t5\t5\n",
      "5\t6\t8\t5\t8\n",
      "4\t9\t8\t4\t7\n",
      "4\t2\t7\t9\t10\n",
      "9\t8\t5\t2\t8\n",
      "8\t5\t7\t8\t4\n",
      "8\t7\t4\t6\t7\n",
      "7\t6\t4\t6\t9\n",
      "3\t6\t11\t5\t7\n",
      "2\t7\t11\t7\t5\n",
      "7\t4\t6\t11\t4\n",
      "10\t6\t6\t6\t4\n",
      "6\t10\t5\t5\t6\n",
      "6\t10\t4\t4\t8\n",
      "6\t2\t10\t8\t6\n",
      "9\t5\t6\t6\t6\n",
      "7\t6\t7\t6\t6\n",
      "8\t2\t7\t5\t10\n",
      "6\t4\t6\t6\t10\n",
      "7\t7\t4\t8\t6\n",
      "6\t7\t7\t7\t5\n",
      "5\t11\t4\t5\t7\n",
      "8\t5\t6\t8\t5\n",
      "7\t5\t5\t7\t8\n",
      "3\t9\t5\t8\t7\n",
      "4\t6\t5\t5\t12\n",
      "5\t9\t9\t4\t5\n",
      "10\t6\t7\t7\t2\n",
      "7\t6\t8\t5\t6\n",
      "7\t10\t5\t3\t7\n",
      "5\t7\t4\t5\t11\n",
      "6\t11\t5\t5\t5\n",
      "5\t8\t8\t6\t5\n",
      "3\t8\t5\t10\t6\n",
      "4\t10\t7\t5\t6\n",
      "9\t2\t6\t5\t10\n",
      "3\t8\t9\t4\t8\n",
      "4\t9\t7\t7\t5\n",
      "8\t5\t9\t8\t2\n",
      "4\t6\t6\t9\t7\n",
      "4\t6\t4\t8\t10\n",
      "6\t2\t8\t8\t8\n",
      "9\t10\t2\t3\t8\n",
      "6\t10\t4\t4\t8\n",
      "3\t8\t9\t6\t6\n",
      "2\t4\t6\t10\t10\n",
      "10\t7\t7\t3\t5\n",
      "6\t5\t10\t7\t4\n",
      "3\t6\t3\t10\t10\n",
      "6\t3\t8\t7\t8\n",
      "8\t3\t8\t10\t3\n",
      "6\t9\t9\t3\t5\n",
      "8\t6\t5\t6\t7\n",
      "12\t8\t4\t4\t4\n",
      "4\t9\t4\t10\t5\n",
      "6\t7\t6\t7\t6\n",
      "5\t6\t8\t8\t5\n",
      "3\t6\t8\t7\t8\n",
      "6\t6\t5\t8\t7\n",
      "7\t3\t9\t8\t5\n",
      "7\t5\t10\t3\t7\n",
      "7\t5\t4\t6\t10\n",
      "9\t8\t3\t4\t8\n",
      "4\t3\t2\t9\t14\n",
      "10\t8\t2\t8\t4\n",
      "8\t7\t6\t6\t5\n",
      "9\t11\t3\t3\t6\n",
      "6\t6\t5\t11\t4\n",
      "7\t4\t11\t4\t6\n",
      "4\t3\t8\t10\t7\n",
      "7\t7\t7\t6\t5\n",
      "6\t3\t7\t7\t9\n",
      "12\t5\t6\t6\t3\n",
      "5\t8\t4\t9\t6\n",
      "8\t6\t5\t4\t9\n",
      "6\t4\t7\t7\t8\n",
      "2\t11\t5\t7\t7\n",
      "8\t7\t7\t3\t7\n",
      "7\t3\t7\t8\t7\n",
      "8\t4\t8\t5\t7\n",
      "9\t3\t4\t3\t13\n",
      "5\t5\t6\t12\t4\n",
      "6\t5\t6\t6\t9\n",
      "10\t5\t5\t6\t6\n",
      "5\t11\t4\t6\t6\n",
      "7\t7\t6\t6\t6\n",
      "8\t4\t2\t11\t7\n",
      "8\t5\t10\t7\t2\n",
      "6\t5\t4\t6\t11\n",
      "5\t9\t3\t7\t8\n",
      "1\t6\t10\t6\t9\n",
      "6\t7\t10\t7\t2\n",
      "7\t7\t7\t6\t5\n",
      "5\t9\t7\t6\t5\n",
      "5\t6\t4\t6\t11\n",
      "3\t6\t10\t9\t4\n",
      "6\t7\t7\t5\t7\n",
      "8\t5\t7\t5\t7\n",
      "4\t10\t9\t6\t3\n",
      "8\t12\t4\t7\t1\n",
      "6\t7\t6\t6\t7\n",
      "8\t4\t8\t7\t5\n",
      "3\t8\t9\t5\t7\n",
      "5\t5\t10\t7\t5\n",
      "7\t2\t10\t7\t6\n",
      "2\t5\t8\t7\t10\n",
      "11\t6\t6\t4\t5\n",
      "6\t8\t8\t7\t3\n",
      "10\t1\t4\t14\t3\n",
      "3\t3\t8\t12\t6\n",
      "6\t8\t4\t9\t5\n",
      "7\t7\t7\t5\t6\n",
      "6\t8\t5\t9\t4\n",
      "10\t10\t7\t3\t2\n",
      "7\t5\t5\t9\t6\n",
      "7\t5\t10\t2\t8\n",
      "6\t5\t6\t8\t7\n",
      "7\t7\t8\t6\t4\n",
      "5\t6\t8\t5\t8\n",
      "5\t7\t5\t8\t7\n",
      "7\t5\t8\t4\t8\n",
      "10\t5\t6\t8\t3\n",
      "5\t4\t9\t6\t8\n",
      "9\t3\t6\t10\t4\n",
      "1\t8\t7\t8\t8\n",
      "10\t6\t5\t3\t8\n",
      "4\t6\t11\t6\t5\n",
      "5\t6\t7\t12\t2\n",
      "4\t5\t13\t5\t5\n",
      "2\t9\t5\t5\t11\n",
      "6\t4\t7\t10\t5\n",
      "6\t4\t8\t9\t5\n",
      "8\t4\t8\t6\t6\n",
      "8\t5\t6\t6\t7\n",
      "7\t7\t7\t4\t7\n",
      "7\t7\t8\t6\t4\n",
      "11\t11\t4\t1\t5\n",
      "5\t6\t8\t4\t9\n",
      "2\t11\t3\t10\t6\n",
      "6\t3\t6\t8\t9\n",
      "11\t1\t4\t7\t9\n",
      "6\t7\t4\t8\t7\n",
      "7\t6\t6\t6\t7\n",
      "5\t9\t9\t4\t5\n",
      "7\t6\t10\t5\t4\n",
      "10\t5\t7\t5\t5\n",
      "6\t6\t5\t6\t9\n",
      "8\t3\t5\t7\t9\n",
      "4\t7\t5\t9\t7\n",
      "4\t5\t7\t10\t6\n",
      "8\t9\t6\t3\t6\n",
      "4\t7\t9\t7\t5\n",
      "4\t10\t9\t4\t5\n",
      "5\t5\t9\t6\t7\n",
      "5\t3\t8\t7\t9\n",
      "8\t6\t10\t1\t7\n",
      "4\t3\t13\t7\t5\n",
      "6\t8\t6\t7\t5\n",
      "6\t7\t7\t7\t5\n",
      "5\t5\t3\t11\t8\n",
      "7\t5\t8\t5\t7\n",
      "4\t7\t7\t6\t8\n",
      "7\t7\t6\t7\t5\n",
      "6\t8\t7\t5\t6\n",
      "4\t6\t8\t4\t10\n",
      "7\t7\t4\t9\t5\n",
      "11\t3\t5\t7\t6\n",
      "4\t4\t7\t9\t8\n",
      "6\t8\t8\t6\t4\n",
      "12\t3\t4\t9\t4\n",
      "1\t9\t6\t8\t8\n",
      "3\t9\t5\t7\t8\n",
      "8\t6\t5\t8\t5\n",
      "8\t2\t8\t4\t10\n",
      "8\t8\t7\t7\t2\n",
      "10\t4\t4\t5\t9\n",
      "6\t3\t7\t7\t9\n",
      "4\t8\t5\t6\t9\n",
      "6\t6\t8\t6\t6\n",
      "9\t8\t7\t2\t6\n",
      "10\t6\t11\t4\t1\n",
      "1\t10\t7\t6\t8\n",
      "5\t7\t8\t5\t7\n",
      "8\t4\t6\t9\t5\n",
      "9\t4\t4\t11\t4\n",
      "6\t8\t5\t5\t8\n",
      "7\t6\t4\t5\t10\n",
      "6\t7\t6\t7\t6\n",
      "9\t4\t8\t8\t3\n",
      "9\t6\t6\t4\t7\n",
      "8\t1\t4\t10\t9\n",
      "7\t4\t6\t6\t9\n",
      "4\t5\t11\t10\t2\n",
      "7\t9\t6\t9\t1\n",
      "10\t7\t4\t6\t5\n",
      "6\t3\t4\t12\t7\n",
      "8\t5\t8\t5\t6\n",
      "8\t6\t5\t7\t6\n",
      "5\t5\t7\t8\t7\n",
      "10\t7\t5\t6\t4\n",
      "7\t8\t4\t4\t9\n",
      "5\t5\t8\t7\t7\n",
      "6\t6\t3\t7\t10\n",
      "7\t6\t6\t7\t6\n",
      "4\t6\t6\t9\t7\n",
      "9\t6\t5\t3\t9\n",
      "3\t5\t7\t8\t9\n",
      "6\t7\t4\t11\t4\n",
      "11\t6\t4\t5\t6\n",
      "4\t9\t6\t7\t6\n",
      "5\t6\t4\t11\t6\n",
      "2\t5\t10\t6\t9\n",
      "6\t6\t8\t7\t5\n",
      "8\t3\t4\t9\t8\n",
      "4\t8\t5\t6\t9\n",
      "8\t4\t6\t8\t6\n",
      "1\t10\t9\t5\t7\n",
      "3\t8\t9\t9\t3\n",
      "5\t4\t7\t9\t7\n",
      "6\t8\t6\t6\t6\n",
      "7\t8\t3\t2\t12\n",
      "10\t7\t3\t4\t8\n",
      "6\t8\t8\t4\t6\n",
      "6\t9\t8\t4\t5\n",
      "7\t6\t7\t5\t7\n",
      "5\t8\t3\t8\t8\n",
      "5\t7\t7\t6\t7\n",
      "9\t6\t2\t7\t8\n",
      "10\t5\t7\t8\t2\n",
      "4\t5\t7\t7\t9\n",
      "8\t6\t6\t7\t5\n",
      "6\t10\t6\t6\t4\n",
      "6\t10\t4\t9\t3\n",
      "9\t6\t4\t4\t9\n",
      "6\t5\t9\t4\t8\n",
      "6\t3\t6\t5\t12\n",
      "2\t13\t5\t4\t8\n",
      "7\t7\t9\t7\t2\n",
      "4\t7\t9\t5\t7\n",
      "9\t7\t7\t4\t5\n",
      "8\t3\t6\t9\t6\n",
      "11\t7\t6\t5\t3\n",
      "4\t6\t12\t4\t6\n",
      "8\t10\t3\t2\t9\n",
      "6\t7\t6\t3\t10\n",
      "5\t5\t6\t8\t8\n",
      "9\t3\t6\t7\t7\n",
      "8\t10\t4\t3\t7\n",
      "7\t8\t3\t6\t8\n",
      "8\t5\t9\t6\t4\n",
      "6\t5\t5\t5\t11\n",
      "9\t8\t2\t7\t6\n",
      "4\t6\t9\t6\t7\n",
      "2\t10\t6\t5\t9\n",
      "7\t8\t6\t2\t9\n",
      "6\t9\t5\t7\t5\n",
      "6\t6\t8\t5\t7\n",
      "5\t7\t6\t7\t7\n",
      "8\t4\t4\t11\t5\n",
      "2\t4\t7\t12\t7\n",
      "4\t6\t8\t6\t8\n",
      "2\t6\t10\t5\t9\n",
      "6\t3\t10\t8\t5\n",
      "8\t6\t7\t5\t6\n",
      "5\t6\t3\t11\t7\n",
      "5\t3\t10\t8\t6\n",
      "3\t9\t8\t8\t4\n",
      "5\t9\t6\t4\t8\n",
      "7\t7\t9\t6\t3\n",
      "7\t6\t7\t8\t4\n",
      "11\t5\t4\t5\t7\n",
      "5\t6\t7\t2\t12\n",
      "6\t9\t7\t5\t5\n",
      "4\t7\t3\t9\t9\n",
      "5\t10\t4\t7\t6\n",
      "7\t3\t9\t6\t7\n",
      "7\t9\t6\t4\t6\n",
      "9\t7\t4\t6\t6\n",
      "8\t7\t3\t6\t8\n",
      "4\t7\t10\t6\t5\n",
      "6\t10\t7\t4\t5\n",
      "8\t6\t5\t8\t5\n",
      "4\t12\t5\t5\t6\n",
      "7\t3\t6\t8\t8\n",
      "8\t2\t8\t7\t7\n",
      "8\t4\t11\t7\t2\n",
      "3\t6\t3\t8\t12\n",
      "10\t3\t5\t8\t6\n",
      "7\t8\t4\t6\t7\n",
      "7\t4\t8\t3\t10\n",
      "2\t8\t12\t3\t7\n",
      "3\t7\t6\t9\t7\n",
      "8\t7\t5\t7\t5\n",
      "10\t3\t7\t10\t2\n",
      "6\t5\t12\t0\t9\n",
      "4\t10\t5\t6\t7\n",
      "5\t6\t8\t8\t5\n",
      "3\t6\t9\t8\t6\n",
      "4\t7\t10\t7\t4\n",
      "8\t6\t8\t7\t3\n",
      "6\t8\t6\t4\t8\n",
      "8\t5\t3\t10\t6\n",
      "10\t4\t9\t6\t3\n",
      "5\t7\t6\t5\t9\n",
      "3\t6\t7\t10\t6\n",
      "5\t5\t6\t12\t4\n",
      "3\t6\t5\t7\t11\n",
      "5\t5\t5\t8\t9\n",
      "5\t3\t7\t8\t9\n",
      "5\t4\t10\t4\t9\n",
      "9\t8\t7\t4\t4\n",
      "7\t6\t8\t4\t7\n",
      "5\t7\t8\t5\t7\n",
      "5\t11\t6\t5\t5\n",
      "11\t5\t8\t4\t4\n",
      "8\t3\t8\t10\t3\n",
      "6\t5\t7\t8\t6\n",
      "7\t4\t9\t7\t5\n",
      "6\t6\t8\t6\t6\n",
      "4\t6\t6\t9\t7\n",
      "1\t10\t1\t8\t12\n",
      "7\t5\t10\t7\t3\n",
      "3\t6\t10\t9\t4\n",
      "5\t9\t6\t7\t5\n",
      "8\t7\t7\t4\t6\n",
      "6\t5\t8\t5\t8\n",
      "4\t6\t12\t6\t4\n",
      "6\t11\t6\t5\t4\n",
      "2\t8\t10\t3\t9\n",
      "8\t3\t10\t4\t7\n",
      "10\t4\t6\t4\t8\n",
      "5\t6\t12\t2\t7\n",
      "9\t4\t7\t7\t5\n",
      "8\t7\t1\t13\t3\n",
      "7\t8\t8\t6\t3\n",
      "7\t4\t6\t10\t5\n",
      "7\t4\t8\t7\t6\n",
      "9\t8\t6\t3\t6\n",
      "9\t10\t3\t8\t2\n",
      "4\t4\t4\t10\t10\n",
      "7\t7\t5\t7\t6\n",
      "5\t5\t9\t7\t6\n",
      "7\t7\t4\t3\t11\n",
      "8\t3\t11\t4\t6\n",
      "5\t6\t5\t9\t7\n",
      "6\t6\t6\t5\t9\n",
      "9\t3\t8\t4\t8\n",
      "7\t3\t9\t6\t7\n",
      "11\t4\t4\t6\t7\n",
      "6\t7\t3\t6\t10\n",
      "6\t7\t5\t6\t8\n",
      "0\t5\t8\t10\t9\n",
      "5\t7\t2\t9\t9\n",
      "8\t8\t5\t4\t7\n",
      "4\t6\t10\t2\t10\n",
      "4\t7\t10\t8\t3\n",
      "8\t4\t6\t6\t8\n",
      "8\t10\t5\t7\t2\n",
      "5\t8\t5\t9\t5\n",
      "7\t6\t9\t4\t6\n",
      "5\t6\t6\t9\t6\n",
      "9\t6\t6\t4\t7\n",
      "3\t7\t8\t8\t6\n",
      "6\t7\t8\t6\t5\n",
      "5\t9\t4\t5\t9\n",
      "11\t9\t4\t4\t4\n",
      "5\t7\t8\t6\t6\n",
      "4\t4\t8\t9\t7\n",
      "10\t6\t2\t8\t6\n",
      "9\t7\t6\t4\t6\n",
      "7\t8\t9\t6\t2\n",
      "5\t6\t5\t4\t12\n",
      "5\t9\t4\t7\t7\n",
      "10\t8\t5\t5\t4\n",
      "4\t8\t8\t6\t6\n",
      "11\t4\t6\t8\t3\n",
      "6\t3\t5\t11\t7\n",
      "5\t6\t7\t5\t9\n",
      "5\t8\t8\t5\t6\n",
      "6\t7\t7\t4\t8\n",
      "6\t4\t7\t12\t3\n",
      "6\t4\t7\t8\t7\n",
      "4\t6\t8\t5\t9\n",
      "5\t9\t4\t8\t6\n",
      "8\t9\t6\t5\t4\n",
      "8\t7\t7\t4\t6\n",
      "6\t4\t5\t11\t6\n",
      "6\t5\t8\t10\t3\n",
      "9\t2\t4\t6\t11\n",
      "6\t4\t7\t6\t9\n",
      "6\t7\t8\t6\t5\n",
      "9\t5\t9\t4\t5\n",
      "6\t5\t7\t9\t5\n",
      "12\t5\t3\t7\t5\n",
      "4\t7\t10\t6\t5\n",
      "5\t5\t9\t3\t10\n",
      "4\t11\t4\t5\t8\n",
      "7\t3\t7\t7\t8\n",
      "4\t4\t12\t4\t8\n",
      "4\t6\t9\t6\t7\n",
      "5\t8\t8\t9\t2\n",
      "6\t5\t1\t12\t8\n",
      "4\t9\t10\t3\t6\n",
      "6\t6\t7\t4\t9\n",
      "6\t12\t4\t5\t5\n",
      "3\t5\t6\t9\t9\n",
      "4\t13\t6\t3\t6\n",
      "5\t8\t5\t7\t7\n",
      "4\t8\t3\t8\t9\n",
      "11\t7\t6\t7\t1\n",
      "5\t7\t6\t5\t9\n",
      "7\t4\t8\t10\t3\n",
      "4\t7\t9\t6\t6\n",
      "7\t6\t9\t8\t2\n",
      "8\t3\t10\t5\t6\n",
      "6\t13\t5\t3\t5\n",
      "6\t9\t1\t6\t10\n",
      "5\t3\t9\t8\t7\n",
      "9\t6\t8\t4\t5\n",
      "12\t6\t1\t9\t4\n",
      "3\t11\t6\t4\t8\n",
      "7\t8\t9\t5\t3\n",
      "7\t7\t7\t4\t7\n",
      "5\t7\t7\t4\t9\n",
      "6\t5\t6\t6\t9\n",
      "5\t7\t6\t4\t10\n",
      "8\t4\t5\t9\t6\n",
      "6\t8\t7\t5\t6\n",
      "6\t8\t4\t6\t8\n",
      "5\t7\t5\t8\t7\n",
      "6\t6\t8\t10\t2\n",
      "5\t7\t7\t8\t5\n",
      "7\t6\t6\t5\t8\n",
      "3\t10\t8\t4\t7\n",
      "7\t4\t3\t8\t10\n",
      "3\t9\t5\t10\t5\n",
      "8\t5\t5\t7\t7\n",
      "6\t6\t6\t6\t8\n",
      "7\t7\t4\t7\t7\n",
      "4\t3\t7\t10\t8\n",
      "5\t5\t10\t6\t6\n",
      "6\t6\t7\t3\t10\n",
      "8\t3\t7\t5\t9\n",
      "9\t4\t6\t8\t5\n",
      "10\t5\t5\t5\t7\n",
      "4\t6\t8\t5\t9\n",
      "9\t7\t9\t4\t3\n",
      "10\t8\t6\t6\t2\n",
      "7\t6\t5\t5\t9\n",
      "6\t8\t5\t3\t10\n",
      "6\t12\t5\t5\t4\n",
      "5\t5\t6\t5\t11\n",
      "8\t6\t5\t4\t9\n",
      "4\t8\t6\t7\t7\n",
      "7\t5\t4\t10\t6\n",
      "9\t7\t6\t6\t4\n",
      "9\t6\t9\t4\t4\n",
      "10\t6\t7\t5\t4\n",
      "8\t5\t6\t5\t8\n",
      "6\t5\t7\t7\t7\n",
      "5\t6\t8\t5\t8\n",
      "7\t4\t6\t9\t6\n",
      "6\t9\t5\t5\t7\n",
      "8\t6\t5\t4\t9\n",
      "3\t7\t9\t3\t10\n",
      "5\t8\t2\t7\t10\n",
      "5\t2\t9\t10\t6\n",
      "6\t9\t5\t6\t6\n",
      "10\t5\t3\t6\t8\n",
      "8\t7\t7\t4\t6\n",
      "6\t4\t7\t13\t2\n",
      "5\t6\t7\t6\t8\n",
      "6\t6\t7\t8\t5\n",
      "8\t5\t7\t10\t2\n",
      "4\t8\t7\t9\t4\n",
      "9\t5\t5\t5\t8\n",
      "9\t4\t4\t4\t11\n",
      "5\t8\t4\t6\t9\n",
      "8\t6\t7\t5\t6\n",
      "7\t6\t7\t10\t2\n",
      "6\t6\t10\t2\t8\n",
      "8\t7\t4\t7\t6\n",
      "7\t4\t7\t8\t6\n",
      "10\t6\t5\t9\t2\n",
      "6\t7\t4\t9\t6\n",
      "5\t9\t4\t6\t8\n",
      "7\t7\t10\t2\t6\n",
      "5\t5\t8\t9\t5\n",
      "6\t6\t7\t7\t6\n",
      "4\t8\t8\t6\t6\n",
      "8\t7\t4\t10\t3\n",
      "7\t3\t9\t5\t8\n",
      "11\t8\t4\t3\t6\n",
      "8\t3\t8\t7\t6\n",
      "7\t8\t7\t5\t5\n",
      "6\t13\t3\t7\t3\n",
      "6\t10\t6\t8\t2\n",
      "9\t9\t9\t2\t3\n",
      "9\t4\t7\t5\t7\n",
      "7\t6\t7\t8\t4\n",
      "7\t7\t8\t4\t6\n",
      "4\t7\t6\t9\t6\n",
      "4\t7\t9\t6\t6\n",
      "9\t7\t5\t7\t4\n",
      "8\t5\t7\t5\t7\n",
      "12\t4\t10\t1\t5\n",
      "6\t5\t11\t3\t7\n",
      "8\t7\t2\t8\t7\n",
      "5\t4\t9\t10\t4\n",
      "9\t3\t1\t9\t10\n",
      "6\t5\t9\t5\t7\n",
      "4\t9\t8\t6\t5\n",
      "1\t10\t7\t5\t9\n",
      "6\t2\t7\t5\t12\n",
      "6\t5\t4\t10\t7\n",
      "7\t7\t2\t12\t4\n",
      "8\t6\t5\t3\t10\n",
      "8\t4\t8\t4\t8\n",
      "3\t9\t7\t7\t6\n",
      "3\t10\t4\t7\t8\n",
      "7\t3\t7\t6\t9\n",
      "9\t6\t8\t3\t6\n",
      "5\t6\t7\t5\t9\n",
      "5\t4\t5\t14\t4\n",
      "12\t8\t3\t4\t5\n"
     ]
    }
   ],
   "source": [
    "for i, samples in enumerate(train_loader):\n",
    "    print(len(np.where(samples[1].numpy() == 0)[0]),\n",
    "        len(np.where(samples[1].numpy() == 1)[0]),\n",
    "        len(np.where(samples[1].numpy() == 2)[0]),\n",
    "        len(np.where(samples[1].numpy() == 3)[0]),\n",
    "        len(np.where(samples[1].numpy() == 4)[0]), sep = \"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "model = Feedforward(dataset.X.shape[1], hidden_size, dataset.num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout)\n",
    "criterion = torch.nn.CrossEntropyLoss() #(weight = torch.Tensor([1,0.8,0.3,0.2,0.9]).cuda())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "evaluate_model(model, val_loader, device)\n",
    "model, loss_values = train_model(model, criterion, optimizer, num_epochs, train_loader, device)\n",
    "evaluate_model(model, val_loader, device)\n",
    "plt.plot(loss_values)\n",
    "plt.title(f\"Number of epochs: {num_epochs}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Epoca: 2, patience: 2424, n stop: 3\n",
      "Early stopping at epoch number 2!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       311\n",
      "           1       0.10      1.00      0.19      1117\n",
      "           2       0.00      0.00      0.00      4265\n",
      "           3       0.00      0.00      0.00      4691\n",
      "           4       0.00      0.00      0.00       394\n",
      "\n",
      "    accuracy                           0.10     10778\n",
      "   macro avg       0.02      0.20      0.04     10778\n",
      "weighted avg       0.01      0.10      0.02     10778\n",
      "\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12919/1787445500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#test_model(model, test_loader, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12919/3959256365.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, epochs, data_loader, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m# y_predd = torch.argmax(y_pred, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/mick.perl/DataAnalyticsProject/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    873\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 self._forward_pre_hooks.values()):\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_reproducibility()\n",
    "\n",
    "\n",
    "for config_params in itertools.product(*hyperparams.values()):\n",
    "\t*names, = hyperparams\n",
    "\tfor param in range(len(hyperparams)):\n",
    "\t\tglobals()[names[param]] = config_params[param]\n",
    "\tmodel = Feedforward(dataset.X.shape[1], hidden_size, dataset.num_classes, af_first_layer, af_hidden_layers, af_output_layer, num_hidden_layers, dropout)\n",
    "\tcriterion = loss_function #(weight = torch.Tensor([1,0.8,0.3,0.2,0.9]).cuda())\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\tmodel.to(device)\n",
    "\t#test_model(model, test_loader, device)\n",
    "\tmodel, loss_values = train_model(model, criterion, optimizer, num_epochs, train_loader, device)\n",
    "\ttest_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, model):\n",
    "    row = torch.Tensor([row])\n",
    "    yhat = model(row)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
